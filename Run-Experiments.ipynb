{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test to run the experiments in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import numpy\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import shutil\n",
    "import socket\n",
    "import string\n",
    "import threading\n",
    "import time\n",
    "import urllib.request\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import libcloud\n",
    "import paramiko\n",
    "from dataclasses import dataclass\n",
    "from libcloud.compute.providers import get_driver\n",
    "from libcloud.compute.types import Provider\n",
    "from paramiko.buffered_pipe import PipeTimeout"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure the SSH user (Massoud cause it's the user where the bd folder is)\n",
    "\n",
    "You need to create a GCLOUD-ACCOUNT with an extra key as explained here: https://libcloud.readthedocs.io/en/stable/compute/drivers/gce.html\n",
    "and add it to the GCLOUD-KEY-PATH for the driver. The private RSA key named PKEY you'll find in your .ssh folder,\n",
    "normally with the name gcloud-compute-platform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "SSH_USER = 'am72ghiassi'\n",
    "GCLOUD_ACCOUNT = 'libcloud@qpe-big-dl.iam.gserviceaccount.com'\n",
    "GCLOUD_KEY_PATH = './qpe.json'  # The path to the Service Account Key (a JSON file)\n",
    "GCLOUD_PROJECT = 'qpe-big-dl'  # GCloud project id\n",
    "PKEY = './google_compute_engine'\n",
    "DESIGN_CSV = ''  # The CSV with the experiment design"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ComputeEngine = get_driver(Provider.GCE)\n",
    "\n",
    "driver = ComputeEngine(GCLOUD_ACCOUNT, GCLOUD_KEY_PATH, project=GCLOUD_PROJECT)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the Node definitions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "\n",
    "class Node(ABC):\n",
    "    def __init__(self, driver, name, master=False, masterNode=None):\n",
    "        \"\"\"Basic Node \"\"\"\n",
    "        print(f'Starting node with name {name}')\n",
    "        self.driver = driver\n",
    "        self.name=name\n",
    "        if not master and masterNode == None:\n",
    "            raise ValueError(\"Slave nodes need a master\")\n",
    "        self.master = masterNode\n",
    "        _nodes = self.driver.list_nodes()\n",
    "        for n in _nodes:\n",
    "            if n.name == self.name:\n",
    "                print(f'Found node {n} with name {n.name} and IPs {n.public_ips}, {n.private_ips}')\n",
    "                self.public_ip = n.public_ips[0]\n",
    "                self.private_ip = n.private_ips[0]\n",
    "        self.connected = False\n",
    "\n",
    "        for i in range(5):  # Try 5 times\n",
    "            try:\n",
    "                self.open_ssh()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "            if not self.connected:\n",
    "                raise RuntimeError(f\"Can't connect to node {self.name}\")\n",
    "        self.start_type()\n",
    "\n",
    "    def open_ssh(self):\n",
    "        self.ssh = paramiko.SSHClient()\n",
    "        self.ssh.load_system_host_keys()\n",
    "        self.ssh.set_missing_host_key_policy(paramiko.WarningPolicy())\n",
    "        self.k = paramiko.RSAKey.from_private_key_file(PKEY)\n",
    "        self.ssh.connect(self.public_ip, username='diego', pkey=self.k)\n",
    "        self.connected = True\n",
    "        print(f'Node {self.name} connected via ssh')\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close_ssh()\n",
    "\n",
    "    def close_ssh(self):\n",
    "        self.connected = False\n",
    "        self.ssh.close()\n",
    "\n",
    "    @abstractmethod\n",
    "    def start_type(self):\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class JobOptions:\n",
    "    batch_size: int\n",
    "    max_epochs: int\n",
    "\n",
    "class MasterNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(\n",
    "            f'/home/{SSH_USER}/bd/spark/sbin/start-master.sh')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "    def submit(self, options: JobOptions, filename, timeout=None, blocking=False):\n",
    "\n",
    "\n",
    "        command = f\"\"\"/home/{SSH_USER}/bd/spark/bin/spark-submit --master spark://{self.private_ip}:7077 --driver-cores 1 \\\n",
    "                    --driver-memory 1G --total-executor-cores 2 --executor-cores 1 --executor-memory 1G \\\n",
    "                    --py-files /home/{SSH_USER}/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/{SSH_USER}/bd/mnist/lenet5.py \\\n",
    "                    --properties-file /home/{SSH_USER}/bd/spark/conf/spark-bigdl.conf \\\n",
    "                    --jars /home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.driver.extraClassPath=/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/{SSH_USER}/bd/mnist/lenet5.py \\\n",
    "                    --action train --dataPath /tmp/mnist --batchSize {options.batch_size} --endTriggerNum {options.max_epochs} > {filename}.log\"\"\"\n",
    "\n",
    "        print(command)\n",
    "\n",
    "\n",
    "        # Get the stdout and err out in case we want the command to run blocking\n",
    "        if not blocking:\n",
    "            self.ssh.exec_command(command)\n",
    "\n",
    "        else:\n",
    "            _, stdout, stderr = self.ssh.exec_command(command)\n",
    "\n",
    "            if len(stderr.read()) > 0:\n",
    "                print(f'There were some errors running the experiment {filename}')\n",
    "                print(stdout.read())\n",
    "                print(stderr.read())\n",
    "\n",
    "\n",
    "\n",
    "    def cancel(self):\n",
    "        br = mechanize.Browser()\n",
    "        br.open(f\"http://{self.public_ip}:8080\")\n",
    "\n",
    "        def select_form(form):\n",
    "            return form.attrs.get('action', None) == 'app/kill/'\n",
    "        try:\n",
    "            br.select_form(predicate=select_form)\n",
    "        except mechanize._mechanize.FormNotFoundError:\n",
    "            print(\"FormNotFoundError\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during cancelloing.\")\n",
    "            print(e)\n",
    "        br.submit()\n",
    "\n",
    "\n",
    "class SlaveNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f'/home/{SSH_USER}/bd/spark/sbin/start-slave.sh spark://{self.master.private_ip}:7077')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create the nodes (master and slaves)\n",
    "\n",
    "You just have to introduce the name of the node and it automatically finds it and starts all the\n",
    "daemons necessary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting node with name bigdl\n",
      "Found node <Node: uuid=cca95b2c0671f4fddfc8dd6f0e127b94cd440f20, name=bigdl, state=RUNNING, public_ips=['34.68.44.195'], private_ips=['10.128.0.4'], provider=Google Compute Engine ...> with name bigdl and IPs ['34.68.44.195'], ['10.128.0.4']\n",
      "Node bigdl connected via ssh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\diego\\cs\\qpe\\venv\\lib\\site-packages\\paramiko\\client.py:837: UserWarning: Unknown ssh-ed25519 host key for 34.68.44.195: b'e5229c5319e7fc72c019f36e3751f9bf'\n",
      "  key.get_name(), hostname, hexlify(key.get_fingerprint())\n"
     ]
    }
   ],
   "source": [
    "# Try to connect to the master node\n",
    "master = MasterNode(driver, 'bigdl', master=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting node with name instance-1\n",
      "Found node <Node: uuid=2f0b3b48e49b0ef04c01e71aaa9ef20d8388451b, name=instance-1, state=RUNNING, public_ips=['34.123.237.102'], private_ips=['10.128.0.5'], provider=Google Compute Engine ...> with name instance-1 and IPs ['34.123.237.102'], ['10.128.0.5']\n",
      "Node instance-1 connected via ssh\n",
      "Starting node with name instance-2\n",
      "Found node <Node: uuid=15d8728539302907d0a183542c07f7f925cbbb5f, name=instance-2, state=RUNNING, public_ips=['35.238.147.71'], private_ips=['10.128.0.6'], provider=Google Compute Engine ...> with name instance-2 and IPs ['35.238.147.71'], ['10.128.0.6']\n",
      "Node instance-2 connected via ssh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\diego\\cs\\qpe\\venv\\lib\\site-packages\\paramiko\\client.py:837: UserWarning: Unknown ssh-ed25519 host key for 34.123.237.102: b'840cb1714132cdeec4f9156e7f3a4dd5'\n",
      "  key.get_name(), hostname, hexlify(key.get_fingerprint())\n",
      "c:\\users\\diego\\cs\\qpe\\venv\\lib\\site-packages\\paramiko\\client.py:837: UserWarning: Unknown ssh-ed25519 host key for 35.238.147.71: b'2152256e2df87da29e08d4ff72221d31'\n",
      "  key.get_name(), hostname, hexlify(key.get_fingerprint())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try to create the other slaves\n",
    "s1 = SlaveNode(driver, 'instance-1', master=False, masterNode=master)\n",
    "s2 = SlaveNode(driver, 'instance-2', master=False, masterNode=master)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Create a list of nodes so we can run the commands on all of them easily\n",
    "from typing import List\n",
    "nodes :List[Node]= []\n",
    "nodes.extend([master,s1,s2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Read the csv with the design and run all the experiments in a loop\n",
    "\n",
    "We need to iterate the experiments and wait for the previous ones to complete\n",
    "\n",
    "The format of the CSV with the experiments is\n",
    "\n",
    "exp-number, batch-size, executor-cores, epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-batch32-epochs1\n",
      "Executing command in bigdl...\n",
      "Executing command in instance-1...\n",
      "Executing command in instance-2...\n",
      "Submitting job to the master with batch 32 and 1 epochs\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.4:7077 --driver-cores 1                     --driver-memory 1G --total-executor-cores 2 --executor-cores 1 --executor-memory 1G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 32 --endTriggerNum 1 > 0-batch32-epochs1.log\n",
      "1-batch64-epochs1\n",
      "Executing command in bigdl...\n",
      "Executing command in instance-1...\n",
      "Executing command in instance-2...\n",
      "Submitting job to the master with batch 64 and 1 epochs\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.4:7077 --driver-cores 1                     --driver-memory 1G --total-executor-cores 2 --executor-cores 1 --executor-memory 1G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 64 --endTriggerNum 1 > 1-batch64-epochs1.log\n",
      "2-batch128-epochs1\n",
      "Executing command in bigdl...\n",
      "Executing command in instance-1...\n",
      "Executing command in instance-2...\n",
      "Submitting job to the master with batch 128 and 1 epochs\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.4:7077 --driver-cores 1                     --driver-memory 1G --total-executor-cores 2 --executor-cores 1 --executor-memory 1G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 128 --endTriggerNum 1 > 2-batch128-epochs1.log\n",
      "3-batch256-epochs1\n",
      "Executing command in bigdl...\n",
      "Executing command in instance-1...\n",
      "Executing command in instance-2...\n",
      "Submitting job to the master with batch 256 and 1 epochs\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.4:7077 --driver-cores 1                     --driver-memory 1G --total-executor-cores 2 --executor-cores 1 --executor-memory 1G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 256 --endTriggerNum 1 > 3-batch256-epochs1.log\n"
     ]
    }
   ],
   "source": [
    "# declare the parameters to save the data and the logs\n",
    "user ='diego'\n",
    "exp_folder = f'/home/{user}/experiments/'\n",
    "script_path = f'/home/{user}/cpu_io_stats.py'\n",
    "\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "exp_runtime = 30\n",
    "\n",
    "exp = pd.read_csv('experiments.csv')\n",
    "exp.columns.values[0] = 'Index'\n",
    "exp.set_index('Index')\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for idx, row in exp.iterrows():\n",
    "\n",
    "    # build the JobOptions\n",
    "    job = JobOptions(row.batch, row.epochs)\n",
    "    filename = f'{int(row.Index)}-batch{row.batch}-epochs{row.epochs}'\n",
    "    print(filename)\n",
    "\n",
    "    # Run the scripts in the multiple nddes\n",
    "    for n in nodes:\n",
    "    # execute the command to get the cpu and io_wait stats\n",
    "        print(f'Executing command in {n.name}...')\n",
    "        n.ssh.exec_command(f\"\"\"python3 {script_path} \\\n",
    "        -o {exp_folder}{filename} -m {exp_runtime} > {exp_folder}{filename}-data.out\"\"\")\n",
    "\n",
    "    # start the scripts in all the nodes\n",
    "    print(f'Submitting job to the master with batch {row.batch} and {row.epochs} epochs')\n",
    "    try:\n",
    "        master.submit(job, filename= filename, timeout=200, blocking=True)\n",
    "    except Exception as e:\n",
    "        print(f'Error in the command:', e)\n",
    "\n",
    "\n",
    "print(f'Experiments finished after {(time.time()-start)/60} minutes')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run the experiment and the script\n",
    "\n",
    "You need to set the account and the location of the cpu-io script so it runs.\n",
    "You also must define the number of seconds of runtime of the script and your experiment folder.\n",
    "\n",
    "the script is run with ```python3 script -o [output_file] -m [max_seconds] -i [interval]```\n",
    "\n",
    "- output file is a pickle file in the experiments folder\n",
    "- max seconds is the run_time of the script. Should be less than the training time of the experiment\n",
    "- interval at which the measurements are taken."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "user ='diego'\n",
    "exp_folder = f'/home/{user}/experiments/'\n",
    "script_path = f'/home/{user}/cpu_io_stats.py'\n",
    "exp_name = 'example'\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "exp_runtime = 100\n",
    "\n",
    "# Submit a job with batch size 128 and 1 epoch for testing\n",
    "opt = JobOptions(256, 1)\n",
    "\n",
    "# Save the output to this file\n",
    "master.submit(opt, '/home/diego/experiments/test.out', 200)\n",
    "\n",
    "# Start the script to gather the cpu and io_wait usage\n",
    "for n in nodes:\n",
    "    # execute the command to get the cpu and io_wait stats\n",
    "    print(f'Executing command in {n.name}...')\n",
    "    n.ssh.exec_command(f\"\"\"python3 {script_path} -o {exp_folder}{exp_name} -m {exp_runtime} > {exp_folder}{exp_name}-data.out\"\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}