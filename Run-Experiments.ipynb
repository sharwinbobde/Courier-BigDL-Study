{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test to run the experiments in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import numpy\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "import socket\n",
    "import string\n",
    "import threading\n",
    "import time\n",
    "import urllib.request\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import libcloud\n",
    "import paramiko\n",
    "from dataclasses import dataclass\n",
    "from libcloud.compute.providers import get_driver\n",
    "from libcloud.compute.types import Provider\n",
    "from paramiko.buffered_pipe import PipeTimeout"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure the SSH user (Massoud cause it's the user where the bd folder is)\n",
    "\n",
    "You need to create a GCLOUD-ACCOUNT with an extra key as explained here: https://libcloud.readthedocs.io/en/stable/compute/drivers/gce.html\n",
    "and add it to the GCLOUD-KEY-PATH for the driver. The private RSA key named PKEY you'll find in your .ssh folder,\n",
    "normally with the name gcloud-compute-platform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SSH_USER = 'am72ghiassi'\n",
    "GCLOUD_ACCOUNT = 'libcloud@qpe-big-dl.iam.gserviceaccount.com'\n",
    "GCLOUD_KEY_PATH = './qpe.json'  # The path to the Service Account Key (a JSON file)\n",
    "GCLOUD_PROJECT = 'qpe-big-dl'  # GCloud project id\n",
    "PKEY = './google_compute_engine'\n",
    "DESIGN_CSV = ''  # The CSV with the experiment design"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ComputeEngine = get_driver(Provider.GCE)\n",
    "\n",
    "driver = ComputeEngine(GCLOUD_ACCOUNT, GCLOUD_KEY_PATH, project=GCLOUD_PROJECT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "driver.list_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the Node definitions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class Node(ABC):\n",
    "    def __init__(self, driver, name, master=False, masterNode=None):\n",
    "        \"\"\"Basic Node \"\"\"\n",
    "        print(f'Starting node with name {name}')\n",
    "        self.driver = driver\n",
    "        self.name=name\n",
    "        if not master and masterNode == None:\n",
    "            raise ValueError(\"Slave nodes need a master\")\n",
    "        self.master = masterNode\n",
    "        _nodes = self.driver.list_nodes()\n",
    "        for n in _nodes:\n",
    "            if n.name == self.name:\n",
    "                print(f'Found node {n} with name {n.name} and IPs {n.public_ips}, {n.private_ips}')\n",
    "                self.public_ip = n.public_ips[0]\n",
    "                self.private_ip = n.private_ips[0]\n",
    "        self.connected = False\n",
    "\n",
    "        for i in range(5):  # Try 5 times\n",
    "            try:\n",
    "                self.open_ssh()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "            if not self.connected:\n",
    "                raise RuntimeError(f\"Can't connect to node {self.name}\")\n",
    "        self.start_type()\n",
    "\n",
    "    def open_ssh(self):\n",
    "        self.ssh = paramiko.SSHClient()\n",
    "        self.ssh.load_system_host_keys()\n",
    "        self.ssh.set_missing_host_key_policy(paramiko.WarningPolicy())\n",
    "        self.k = paramiko.RSAKey.from_private_key_file(PKEY)\n",
    "        self.ssh.connect(self.public_ip, username='diego', pkey=self.k)\n",
    "        self.connected = True\n",
    "        print(f'Node {self.name} connected via ssh')\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close_ssh()\n",
    "\n",
    "    def close_ssh(self):\n",
    "        self.connected = False\n",
    "        self.ssh.close()\n",
    "\n",
    "    @abstractmethod\n",
    "    def start_type(self):\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class JobOptions:\n",
    "    core_number: int\n",
    "    batch_size: int\n",
    "    max_epochs: int\n",
    "\n",
    "class MasterNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(\n",
    "            f'/home/{SSH_USER}/bd/spark/sbin/start-master.sh')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "    def submit(self, options: JobOptions, filename, save_path:str, timeout=None, blocking=False):\n",
    "\n",
    "\n",
    "        command = f\"\"\"/home/{SSH_USER}/bd/spark/bin/spark-submit --master spark://{self.private_ip}:7077 --driver-cores 4 \\\n",
    "                    --driver-memory 6G --total-executor-cores {options.core_number} --executor-cores 1 --executor-memory 4G \\\n",
    "                    --py-files /home/{SSH_USER}/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/{SSH_USER}/bd/mnist/lenet5.py \\\n",
    "                    --properties-file /home/{SSH_USER}/bd/spark/conf/spark-bigdl.conf \\\n",
    "                    --jars /home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.driver.extraClassPath=/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/{SSH_USER}/bd/mnist/lenet5.py \\\n",
    "                    --action train --dataPath /tmp/mnist --batchSize {options.batch_size} --endTriggerNum {options.max_epochs} > {save_path}{filename}.log\"\"\"\n",
    "\n",
    "        print(command)\n",
    "\n",
    "\n",
    "        # Get the stdout and err out in case we want the command to run blocking\n",
    "        if not blocking:\n",
    "            self.ssh.exec_command(command)\n",
    "\n",
    "        else:\n",
    "            _, stdout, stderr = self.ssh.exec_command(command)\n",
    "\n",
    "            if len(stderr.read()) > 0:\n",
    "                print(f'There were some errors running the experiment {filename}')\n",
    "                print(stdout.read())\n",
    "                print(stderr.read())\n",
    "\n",
    "\n",
    "\n",
    "    def cancel(self):\n",
    "        br = mechanize.Browser()\n",
    "        br.open(f\"http://{self.public_ip}:8080\")\n",
    "\n",
    "        def select_form(form):\n",
    "            return form.attrs.get('action', None) == 'app/kill/'\n",
    "        try:\n",
    "            br.select_form(predicate=select_form)\n",
    "        except mechanize._mechanize.FormNotFoundError:\n",
    "            print(\"FormNotFoundError\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during cancelloing.\")\n",
    "            print(e)\n",
    "        br.submit()\n",
    "\n",
    "\n",
    "class SlaveNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f'/home/{SSH_USER}/bd/spark/sbin/start-slave.sh spark://{self.master.private_ip}:7077')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create the nodes (master and slaves)\n",
    "\n",
    "You just have to introduce the name of the node and it automatically finds it and starts all the\n",
    "daemons necessary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try to connect to the master node\n",
    "master = MasterNode(driver, 'bigdl-master-1', master=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Try to create the other slaves\n",
    "s1 = SlaveNode(driver, 'slave-1', master=False, masterNode=master)\n",
    "s2 = SlaveNode(driver, 'slave-2', master=False, masterNode=master)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of nodes so we can run the commands on all of them easily\n",
    "from typing import List\n",
    "nodes :List[Node]= []\n",
    "nodes.extend([master,s1,s2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####Read the csv with the design and run all the experiments in a loop\n",
    "\n",
    "We need to iterate the experiments and wait for the previous ones to complete\n",
    "The format of the CSV with the experiments is\n",
    "\n",
    "exp-number, cpus, batch-size, njobs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# declare the parameters to save the data and the logs\n",
    "user ='diego'\n",
    "exp_folder = f'/home/{user}/experiments/'\n",
    "script_path = f'/home/{user}/cpu_io_stats.py'\n",
    "\n",
    "# experiment designs\n",
    "factorial_2k = 'experiment_designs/2k_design.csv'\n",
    "full_fact = 'experiment_designs/fullfact.csv'\n",
    "\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "EXP_RUNTIME = 50\n",
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "# read the file that we're interested in\n",
    "exp = pd.read_csv(factorial_2k, dtype=int)\n",
    "exp.columns.values[0] = 'Index'\n",
    "exp.set_index('Index')\n",
    "\n",
    "exp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the experiments in a loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove previous experiments to make room for the new (with a dialog for safety)\n",
    "response = str(input('Removing previous experiments from the servers, continue? (y/N)'))\n",
    "if response.lower() != 'y':\n",
    "    print('stopping...')\n",
    "    # quit the execution \"nicely\" without stopping the kernel\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "# Run the scripts in the multiple nddes\n",
    "for n in nodes:\n",
    "    # Create the folder for the experiment\n",
    "\n",
    "    print('Removing previous experiments...')\n",
    "    _, _, stderr = n.ssh.exec_command(f'rm -rf {exp_folder}*')\n",
    "    if len(stderr.read()) != 0:\n",
    "        print('Error creating folder', stderr.read())\n",
    "        exit(-1)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for idx, row in exp.iterrows():\n",
    "\n",
    "    # build the JobOptions with 30 epochs by default\n",
    "    job = JobOptions(core_number=row.cpu, batch_size=row.batch, max_epochs=EPOCHS)\n",
    "    filename = f'{int(row.Index)}-cpu{row.cpu}-batch{row.batch}-njobs{row.njobs}'\n",
    "    print(filename)\n",
    "\n",
    "    for n in nodes:\n",
    "        print('Creating folder in', n.name)\n",
    "        _, _, stderr = n.ssh.exec_command(f'mkdir {exp_folder}{filename}')\n",
    "        if len(stderr.read()) != 0:\n",
    "            print('Error creating folder', stderr.read())\n",
    "            exit(-1)\n",
    "\n",
    "        # execute the command to get the cpu and io_wait stats\n",
    "        print(f'Executing command in {n.name}...')\n",
    "        n.ssh.exec_command(f\"\"\"python3 {script_path} \\\n",
    "        -o {exp_folder}{filename}/{filename} -m {EXP_RUNTIME} > {exp_folder}{filename}/{filename}-data.out\"\"\")\n",
    "\n",
    "\n",
    "    njobs = row.njobs\n",
    "    # submit job to the master\n",
    "    print(f'Submitting {njobs} job(s) to the master with batch {row.batch} and {row.cpu} cores')\n",
    "\n",
    "    try:\n",
    "        for i in range(njobs):\n",
    "            # Only block after submitting all of the tasks\n",
    "            blocking = False if i < (njobs-1) else True\n",
    "            print(f'Task number {i}, blocking = {blocking}')\n",
    "            master.submit(job, save_path = f'{exp_folder}{filename}/', filename= filename, timeout=200, blocking=blocking)\n",
    "    except Exception as e:\n",
    "        print(f'Error in the command:', e)\n",
    "        master.cancel()\n",
    "\n",
    "\n",
    "print(f'Experiments finished after {(time.time()-start)/60} minutes')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run the experiment and the script\n",
    "\n",
    "You need to set the account and the location of the cpu-io script so it runs.\n",
    "You also must define the number of seconds of runtime of the script and your experiment folder.\n",
    "\n",
    "the script is run with ```python3 script -o [output_file] -m [max_seconds] -i [interval]```\n",
    "\n",
    "- output file is a pickle file in the experiments folder\n",
    "- max seconds is the run_time of the script. Should be less than the training time of the experiment\n",
    "- interval at which the measurements are taken."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "user ='diego'\n",
    "exp_folder = f'/home/{user}/experiments/'\n",
    "script_path = f'/home/{user}/cpu_io_stats.py'\n",
    "exp_name = 'example'\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "exp_runtime = 100\n",
    "\n",
    "# Submit a job with batch size 128 and 1 epoch for testing\n",
    "opt = JobOptions(256, 1)\n",
    "\n",
    "# Save the output to this file\n",
    "master.submit(opt, '/home/diego/experiments/test.out', 200)\n",
    "\n",
    "# Start the script to gather the cpu and io_wait usage\n",
    "for n in nodes:\n",
    "    # execute the command to get the cpu and io_wait stats\n",
    "    print(f'Executing command in {n.name}...')\n",
    "    n.ssh.exec_command(f\"\"\"python3 {script_path} -o {exp_folder}{exp_name} -m {exp_runtime} > {exp_folder}{exp_name}-data.out\"\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}