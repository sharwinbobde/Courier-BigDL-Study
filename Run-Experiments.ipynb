{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test to run the experiments in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import numpy\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "import socket\n",
    "import string\n",
    "import threading\n",
    "import time\n",
    "import urllib.request\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import libcloud\n",
    "import paramiko\n",
    "from dataclasses import dataclass\n",
    "from libcloud.compute.providers import get_driver\n",
    "from libcloud.compute.types import Provider\n",
    "from paramiko.buffered_pipe import PipeTimeout"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure the SSH user (Massoud cause it's the user where the bd folder is)\n",
    "\n",
    "You need to create a GCLOUD-ACCOUNT with an extra key as explained here: https://libcloud.readthedocs.io/en/stable/compute/drivers/gce.html\n",
    "and add it to the GCLOUD-KEY-PATH for the driver. The private RSA key named PKEY you'll find in your .ssh folder,\n",
    "normally with the name gcloud-compute-platform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "SSH_USER = 'am72ghiassi'\n",
    "GCLOUD_ACCOUNT = 'libcloud@qpe-big-dl.iam.gserviceaccount.com'\n",
    "GCLOUD_KEY_PATH = './qpe.json'  # The path to the Service Account Key (a JSON file)\n",
    "GCLOUD_PROJECT = 'qpe-big-dl'  # GCloud project id\n",
    "PKEY = './google_compute_engine'\n",
    "DESIGN_CSV = ''  # The CSV with the experiment design"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ComputeEngine = get_driver(Provider.GCE)\n",
    "\n",
    "driver = ComputeEngine(GCLOUD_ACCOUNT, GCLOUD_KEY_PATH, project=GCLOUD_PROJECT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[<Node: uuid=cca95b2c0671f4fddfc8dd6f0e127b94cd440f20, name=bigdl, state=STOPPED, public_ips=[None], private_ips=['10.128.0.4'], provider=Google Compute Engine ...>,\n <Node: uuid=3124f0c0c1eaf12f9aeef33c7d532c7952535b20, name=bigdl-master-1, state=RUNNING, public_ips=['34.67.204.85'], private_ips=['10.128.0.11'], provider=Google Compute Engine ...>,\n <Node: uuid=2f0b3b48e49b0ef04c01e71aaa9ef20d8388451b, name=instance-1, state=STOPPED, public_ips=[None], private_ips=['10.128.0.5'], provider=Google Compute Engine ...>,\n <Node: uuid=15d8728539302907d0a183542c07f7f925cbbb5f, name=instance-2, state=STOPPED, public_ips=[None], private_ips=['10.128.0.6'], provider=Google Compute Engine ...>,\n <Node: uuid=9980784de1b7fe98f2b79b2296109687f6ad7ca3, name=slave-1, state=RUNNING, public_ips=['35.239.159.190'], private_ips=['10.128.0.8'], provider=Google Compute Engine ...>,\n <Node: uuid=50752b239c049bc2d396c687607532b2b9fa1378, name=slave-2, state=RUNNING, public_ips=['34.123.251.206'], private_ips=['10.128.0.10'], provider=Google Compute Engine ...>,\n <Node: uuid=6abb65747e2d4a3af4ad62bca2aa2a9edcf633fa, name=slave-3, state=RUNNING, public_ips=['34.123.211.155'], private_ips=['10.128.0.12'], provider=Google Compute Engine ...>,\n <Node: uuid=ebf17e954a59cfd880ebc56d46a3427175768442, name=slave-4, state=RUNNING, public_ips=['35.224.46.25'], private_ips=['10.128.0.13'], provider=Google Compute Engine ...>]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.list_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the Node definitions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "class Node(ABC):\n",
    "    def __init__(self, driver, name, master=False, masterNode=None):\n",
    "        \"\"\"Basic Node \"\"\"\n",
    "        print(f'Starting node with name {name}')\n",
    "        self.driver = driver\n",
    "        self.name=name\n",
    "        if not master and masterNode == None:\n",
    "            raise ValueError(\"Slave nodes need a master\")\n",
    "        self.master = masterNode\n",
    "        _nodes = self.driver.list_nodes()\n",
    "        for n in _nodes:\n",
    "            if n.name == self.name:\n",
    "                print(f'Found node {n} with name {n.name} and IPs {n.public_ips}, {n.private_ips}')\n",
    "                self.public_ip = n.public_ips[0]\n",
    "                self.private_ip = n.private_ips[0]\n",
    "        self.connected = False\n",
    "\n",
    "        for i in range(5):  # Try 5 times\n",
    "            try:\n",
    "                self.open_ssh()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "            if not self.connected:\n",
    "                raise RuntimeError(f\"Can't connect to node {self.name}\")\n",
    "        self.start_type()\n",
    "\n",
    "    def open_ssh(self):\n",
    "        self.ssh = paramiko.SSHClient()\n",
    "        self.ssh.load_system_host_keys()\n",
    "        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        self.k = paramiko.RSAKey.from_private_key_file(PKEY)\n",
    "        self.ssh.connect(self.public_ip, username='diego', pkey=self.k)\n",
    "        self.connected = True\n",
    "        print(f'Node {self.name} connected via ssh')\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close_ssh()\n",
    "\n",
    "    def close_ssh(self):\n",
    "        self.connected = False\n",
    "        self.ssh.close()\n",
    "\n",
    "    @abstractmethod\n",
    "    def start_type(self):\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class JobOptions:\n",
    "    core_number: int\n",
    "    batch_size: int\n",
    "    max_epochs: int\n",
    "\n",
    "class MasterNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(\n",
    "            f'/home/{SSH_USER}/bd/spark/sbin/start-master.sh')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "    def submit(self, options: JobOptions, filename, save_path:str, timeout=None, blocking=False):\n",
    "\n",
    "\n",
    "        command = f\"\"\"/home/{SSH_USER}/bd/spark/bin/spark-submit --master spark://{self.private_ip}:7077 --driver-cores 4 \\\n",
    "                    --driver-memory 6G --total-executor-cores {options.core_number} --executor-cores 1 --executor-memory 3G \\\n",
    "                    --py-files /home/{SSH_USER}/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/{SSH_USER}/bd/mnist/lenet5.py \\\n",
    "                    --properties-file /home/{SSH_USER}/bd/spark/conf/spark-bigdl.conf \\\n",
    "                    --jars /home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.driver.extraClassPath=/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/{SSH_USER}/bd/mnist/lenet5.py \\\n",
    "                    --action train --dataPath /tmp/mnist --batchSize {options.batch_size} --endTriggerNum {options.max_epochs} > {save_path}{filename}.log\"\"\"\n",
    "\n",
    "        print(command)\n",
    "\n",
    "\n",
    "        # Get the stdout and err out in case we want the command to run blocking\n",
    "        if not blocking:\n",
    "            self.ssh.exec_command(command)\n",
    "\n",
    "        else:\n",
    "            _, stdout, stderr = self.ssh.exec_command(command)\n",
    "\n",
    "            if len(stderr.read()) > 0:\n",
    "                print(f'There were some errors running the experiment {filename}')\n",
    "                print(stdout.read())\n",
    "                print(stderr.read())\n",
    "\n",
    "\n",
    "\n",
    "    def cancel(self):\n",
    "        br = mechanize.Browser()\n",
    "        br.open(f\"http://{self.public_ip}:8080\")\n",
    "\n",
    "        def select_form(form):\n",
    "            return form.attrs.get('action', None) == 'app/kill/'\n",
    "        try:\n",
    "            br.select_form(predicate=select_form)\n",
    "        except mechanize._mechanize.FormNotFoundError:\n",
    "            print(\"FormNotFoundError\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during cancelloing.\")\n",
    "            print(e)\n",
    "        br.submit()\n",
    "\n",
    "\n",
    "class SlaveNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f'/home/{SSH_USER}/bd/spark/sbin/start-slave.sh spark://{self.master.private_ip}:7077')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create the nodes (master and slaves)\n",
    "\n",
    "You just have to introduce the name of the node and it automatically finds it and starts all the\n",
    "daemons necessary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting node with name bigdl-master-1\n",
      "Found node <Node: uuid=3124f0c0c1eaf12f9aeef33c7d532c7952535b20, name=bigdl-master-1, state=RUNNING, public_ips=['34.67.204.85'], private_ips=['10.128.0.11'], provider=Google Compute Engine ...> with name bigdl-master-1 and IPs ['34.67.204.85'], ['10.128.0.11']\n",
      "Node bigdl-master-1 connected via ssh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method Node.__del__ of <__main__.MasterNode object at 0x000001E2A51264A8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-21-075259c6d7cf>\", line 39, in __del__\n",
      "  File \"<ipython-input-21-075259c6d7cf>\", line 43, in close_ssh\n",
      "AttributeError: 'MasterNode' object has no attribute 'ssh'\n"
     ]
    }
   ],
   "source": [
    "# Try to connect to the master node\n",
    "master = MasterNode(driver, 'bigdl-master-1', master=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting node with name slave-1\n",
      "Found node <Node: uuid=9980784de1b7fe98f2b79b2296109687f6ad7ca3, name=slave-1, state=RUNNING, public_ips=['35.239.159.190'], private_ips=['10.128.0.8'], provider=Google Compute Engine ...> with name slave-1 and IPs ['35.239.159.190'], ['10.128.0.8']\n",
      "Node slave-1 connected via ssh\n",
      "Starting node with name slave-2\n",
      "Found node <Node: uuid=50752b239c049bc2d396c687607532b2b9fa1378, name=slave-2, state=RUNNING, public_ips=['34.123.251.206'], private_ips=['10.128.0.10'], provider=Google Compute Engine ...> with name slave-2 and IPs ['34.123.251.206'], ['10.128.0.10']\n",
      "Node slave-2 connected via ssh\n",
      "Starting node with name slave-3\n",
      "Found node <Node: uuid=6abb65747e2d4a3af4ad62bca2aa2a9edcf633fa, name=slave-3, state=RUNNING, public_ips=['34.123.211.155'], private_ips=['10.128.0.12'], provider=Google Compute Engine ...> with name slave-3 and IPs ['34.123.211.155'], ['10.128.0.12']\n",
      "Node slave-3 connected via ssh\n",
      "Starting node with name slave-4\n",
      "Found node <Node: uuid=ebf17e954a59cfd880ebc56d46a3427175768442, name=slave-4, state=RUNNING, public_ips=['35.224.46.25'], private_ips=['10.128.0.13'], provider=Google Compute Engine ...> with name slave-4 and IPs ['35.224.46.25'], ['10.128.0.13']\n",
      "Node slave-4 connected via ssh\n",
      "Starting node with name slave-5\n",
      "Found node <Node: uuid=fbcb85123d7454b6fea4958aec86012d77f97258, name=slave-5, state=RUNNING, public_ips=['35.192.100.6'], private_ips=['10.128.0.14'], provider=Google Compute Engine ...> with name slave-5 and IPs ['35.192.100.6'], ['10.128.0.14']\n",
      "Node slave-5 connected via ssh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try to create the other slaves\n",
    "s1 = SlaveNode(driver, 'slave-1', master=False, masterNode=master)\n",
    "s2 = SlaveNode(driver, 'slave-2', master=False, masterNode=master)\n",
    "s3 = SlaveNode(driver, 'slave-3', master=False, masterNode=master)\n",
    "s4 = SlaveNode(driver, 'slave-4', master=False, masterNode=master)\n",
    "s5 = SlaveNode(driver, 'slave-5', master=False, masterNode=master)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Create a list of nodes so we can run the commands on all of them easily\n",
    "from typing import List\n",
    "nodes :List[Node]= []\n",
    "nodes.extend([master,s1,s2,s3,s4, s5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####Read the csv with the design and run all the experiments in a loop\n",
    "\n",
    "We need to iterate the experiments and wait for the previous ones to complete\n",
    "The format of the CSV with the experiments is\n",
    "\n",
    "exp-number, cpus, batch-size, njobs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "   Index  cpu  batch  njobs\n0      0    1     32      1\n1      1    8     32      1\n2      2    1    512      1\n3      3    8    512      1\n4      4    1     32      5\n5      5    8     32      5\n6      6    1    512      5\n7      7    8    512      5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Index</th>\n      <th>cpu</th>\n      <th>batch</th>\n      <th>njobs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>8</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n      <td>512</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>8</td>\n      <td>512</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n      <td>32</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>8</td>\n      <td>32</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>1</td>\n      <td>512</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>8</td>\n      <td>512</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare the parameters to save the data and the logs\n",
    "user ='diego'\n",
    "exp_folder = f'/home/{user}/experiments/'\n",
    "script_path = f'/home/{user}/cpu_io_stats.py'\n",
    "\n",
    "# experiment designs\n",
    "factorial_2k = 'experiment_designs/2k_design.csv'\n",
    "full_fact = 'experiment_designs/fullfact.csv'\n",
    "\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "EXP_RUNTIME = 5 * 60 # 5 minutes runtime by default\n",
    "EPOCHS = 30 # 30 epochs by default\n",
    "\n",
    "\n",
    "# read the file that we're interested in\n",
    "exp = pd.read_csv(factorial_2k, dtype=int)\n",
    "exp.columns.values[0] = 'Index'\n",
    "exp.set_index('Index')\n",
    "\n",
    "exp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the experiments in a loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "1-cpu8-batch32-njobs1\n",
      "Creating folder in bigdl-master-1\n",
      "Executing command in bigdl-master-1...\n",
      "Creating folder in slave-1\n",
      "Executing command in slave-1...\n",
      "Creating folder in slave-2\n",
      "Executing command in slave-2...\n",
      "Creating folder in slave-3\n",
      "Executing command in slave-3...\n",
      "Creating folder in slave-4\n",
      "Executing command in slave-4...\n",
      "Creating folder in slave-5\n",
      "Executing command in slave-5...\n",
      "Submitting 1 job(s) to the master with batch 32 and 8 cores\n",
      "Task number 0, blocking = True\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 32 --endTriggerNum 1 > /home/diego/experiments/1-cpu8-batch32-njobs1/1-cpu8-batch32-njobs1.log\n",
      "3-cpu8-batch512-njobs1\n",
      "Creating folder in bigdl-master-1\n",
      "Executing command in bigdl-master-1...\n",
      "Creating folder in slave-1\n",
      "Executing command in slave-1...\n",
      "Creating folder in slave-2\n",
      "Executing command in slave-2...\n",
      "Creating folder in slave-3\n",
      "Executing command in slave-3...\n",
      "Creating folder in slave-4\n",
      "Executing command in slave-4...\n",
      "Creating folder in slave-5\n",
      "Executing command in slave-5...\n",
      "Submitting 1 job(s) to the master with batch 512 and 8 cores\n",
      "Task number 0, blocking = True\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 1 > /home/diego/experiments/3-cpu8-batch512-njobs1/3-cpu8-batch512-njobs1.log\n",
      "5-cpu8-batch32-njobs5\n",
      "Creating folder in bigdl-master-1\n",
      "Executing command in bigdl-master-1...\n",
      "Creating folder in slave-1\n",
      "Executing command in slave-1...\n",
      "Creating folder in slave-2\n",
      "Executing command in slave-2...\n",
      "Creating folder in slave-3\n",
      "Executing command in slave-3...\n",
      "Creating folder in slave-4\n",
      "Executing command in slave-4...\n",
      "Creating folder in slave-5\n",
      "Executing command in slave-5...\n",
      "Submitting 5 job(s) to the master with batch 32 and 8 cores\n",
      "Task number 0, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 32 --endTriggerNum 1 > /home/diego/experiments/5-cpu8-batch32-njobs5/5-cpu8-batch32-njobs5.log\n",
      "Task number 1, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 32 --endTriggerNum 1 > /home/diego/experiments/5-cpu8-batch32-njobs5/5-cpu8-batch32-njobs5.log\n",
      "Task number 2, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 32 --endTriggerNum 1 > /home/diego/experiments/5-cpu8-batch32-njobs5/5-cpu8-batch32-njobs5.log\n",
      "Task number 3, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 32 --endTriggerNum 1 > /home/diego/experiments/5-cpu8-batch32-njobs5/5-cpu8-batch32-njobs5.log\n",
      "Task number 4, blocking = True\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 32 --endTriggerNum 1 > /home/diego/experiments/5-cpu8-batch32-njobs5/5-cpu8-batch32-njobs5.log\n",
      "7-cpu8-batch512-njobs5\n",
      "Creating folder in bigdl-master-1\n",
      "Executing command in bigdl-master-1...\n",
      "Creating folder in slave-1\n",
      "Executing command in slave-1...\n",
      "Creating folder in slave-2\n",
      "Executing command in slave-2...\n",
      "Creating folder in slave-3\n",
      "Executing command in slave-3...\n",
      "Creating folder in slave-4\n",
      "Executing command in slave-4...\n",
      "Creating folder in slave-5\n",
      "Executing command in slave-5...\n",
      "Submitting 5 job(s) to the master with batch 512 and 8 cores\n",
      "Task number 0, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 1 > /home/diego/experiments/7-cpu8-batch512-njobs5/7-cpu8-batch512-njobs5.log\n",
      "Task number 1, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 1 > /home/diego/experiments/7-cpu8-batch512-njobs5/7-cpu8-batch512-njobs5.log\n",
      "Task number 2, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 1 > /home/diego/experiments/7-cpu8-batch512-njobs5/7-cpu8-batch512-njobs5.log\n",
      "Task number 3, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 1 > /home/diego/experiments/7-cpu8-batch512-njobs5/7-cpu8-batch512-njobs5.log\n",
      "Task number 4, blocking = True\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 1 > /home/diego/experiments/7-cpu8-batch512-njobs5/7-cpu8-batch512-njobs5.log\n",
      "0-cpu1-batch32-njobs1\n",
      "Creating folder in bigdl-master-1\n",
      "Executing command in bigdl-master-1...\n",
      "Creating folder in slave-1\n",
      "Executing command in slave-1...\n",
      "Creating folder in slave-2\n",
      "Executing command in slave-2...\n",
      "Creating folder in slave-3\n",
      "Executing command in slave-3...\n",
      "Creating folder in slave-4\n",
      "Executing command in slave-4...\n",
      "Creating folder in slave-5\n",
      "Executing command in slave-5...\n",
      "Submitting 1 job(s) to the master with batch 32 and 1 cores\n",
      "Task number 0, blocking = True\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 1 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 32 --endTriggerNum 1 > /home/diego/experiments/0-cpu1-batch32-njobs1/0-cpu1-batch32-njobs1.log\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-27-9a00ad167694>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     52\u001B[0m             \u001B[0mblocking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mnjobs\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'Task number {i}, blocking = {blocking}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 54\u001B[1;33m             \u001B[0mmaster\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubmit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msave_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mf'{exp_folder}{filename}/'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m=\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m200\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mblocking\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mblocking\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     55\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     56\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'Error in the command:'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-21-075259c6d7cf>\u001B[0m in \u001B[0;36msubmit\u001B[1;34m(self, options, filename, save_path, timeout, blocking)\u001B[0m\n\u001B[0;32m     83\u001B[0m             \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstdout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstderr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mssh\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexec_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 85\u001B[1;33m             \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstderr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     86\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'There were some errors running the experiment {filename}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     87\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstdout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\diego\\cs\\qpe\\venv\\lib\\site-packages\\paramiko\\file.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    198\u001B[0m             \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 200\u001B[1;33m                     \u001B[0mnew_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_DEFAULT_BUFSIZE\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    201\u001B[0m                 \u001B[1;32mexcept\u001B[0m \u001B[0mEOFError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    202\u001B[0m                     \u001B[0mnew_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\diego\\cs\\qpe\\venv\\lib\\site-packages\\paramiko\\channel.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m   1374\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1375\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msize\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1376\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchannel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecv_stderr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1377\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1378\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_write\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\diego\\cs\\qpe\\venv\\lib\\site-packages\\paramiko\\channel.py\u001B[0m in \u001B[0;36mrecv_stderr\u001B[1;34m(self, nbytes)\u001B[0m\n\u001B[0;32m    745\u001B[0m         \"\"\"\n\u001B[0;32m    746\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 747\u001B[1;33m             \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0min_stderr_buffer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnbytes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    748\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mPipeTimeout\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    749\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\diego\\cs\\qpe\\venv\\lib\\site-packages\\paramiko\\buffered_pipe.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, nbytes, timeout)\u001B[0m\n\u001B[0;32m    158\u001B[0m                 \u001B[1;32mwhile\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_buffer\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_closed\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    159\u001B[0m                     \u001B[0mthen\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 160\u001B[1;33m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_cv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    161\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m                         \u001B[0mtimeout\u001B[0m \u001B[1;33m-=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mthen\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\threading.py\u001B[0m in \u001B[0;36mwait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    293\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m    \u001B[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    294\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 295\u001B[1;33m                 \u001B[0mwaiter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    296\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    297\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Remove previous experiments to make room for the new (with a dialog for safety)\n",
    "response = str(input('Removing previous experiments from the servers, continue? (y/N)'))\n",
    "if response.lower() != 'y':\n",
    "    print('stopping...')\n",
    "    # quit the execution \"nicely\" without stopping the kernel\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "# Run the scripts in the multiple nddes\n",
    "for n in nodes:\n",
    "    # Create the folder for the experiment\n",
    "\n",
    "    print('Removing previous experiments...')\n",
    "    _, _, stderr = n.ssh.exec_command(f'rm -rf {exp_folder}*')\n",
    "    if len(stderr.read()) != 0:\n",
    "        print('Error creating folder', stderr.read())\n",
    "        exit(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for idx, row in exp.iterrows():\n",
    "\n",
    "    # build the JobOptions with 30 epochs by default\n",
    "    job = JobOptions(core_number=row.cpu, batch_size=row.batch, max_epochs=EPOCHS)\n",
    "    filename = f'{int(row.Index)}-cpu{row.cpu}-batch{row.batch}-njobs{row.njobs}'\n",
    "    print('\\n',filename)\n",
    "\n",
    "    for n in nodes:\n",
    "        # print('Creating folder in', n.name)\n",
    "        _, _, stderr = n.ssh.exec_command(f'mkdir {exp_folder}{filename}')\n",
    "        if len(stderr.read()) != 0:\n",
    "            print('Error creating folder', stderr.read())\n",
    "            exit(-1)\n",
    "\n",
    "\n",
    "\n",
    "        # execute the command to get the cpu and io_wait stats\n",
    "        # print(f'Executing command in {n.name}...')\n",
    "        n.ssh.exec_command(f\"\"\"python3 {script_path} \\\n",
    "        -o {exp_folder}{filename}/{filename} -m {EXP_RUNTIME} >{exp_folder}{filename}/{filename}-data.out 2>&1\"\"\")\n",
    "\n",
    "\n",
    "    njobs = row.njobs\n",
    "    # submit job to the master\n",
    "    print(f'Submitting {njobs} job(s) to the master with batch {row.batch} and {row.cpu} cores')\n",
    "\n",
    "    try:\n",
    "        for i in range(njobs):\n",
    "            # Only block after submitting all of the tasks\n",
    "            blocking = False if i < (njobs-1) else True\n",
    "            print(f'Task number {i}, blocking = {blocking}')\n",
    "            master.submit(job, save_path = f'{exp_folder}{filename}/', filename= filename, timeout=200, blocking=blocking)\n",
    "    except Exception as e:\n",
    "        print(f'Error in the command:', e)\n",
    "        master.cancel()\n",
    "\n",
    "\n",
    "print(f'\\nExperiments finished after {(time.time()-start)/60} minutes')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run the experiment and the script\n",
    "\n",
    "You need to set the account and the location of the cpu-io script so it runs.\n",
    "You also must define the number of seconds of runtime of the script and your experiment folder.\n",
    "\n",
    "the script is run with ```python3 script -o [output_file] -m [max_seconds] -i [interval]```\n",
    "\n",
    "- output file is a pickle file in the experiments folder\n",
    "- max seconds is the run_time of the script. Should be less than the training time of the experiment\n",
    "- interval at which the measurements are taken."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "user ='diego'\n",
    "exp_folder = f'/home/{user}/experiments/'\n",
    "script_path = f'/home/{user}/cpu_io_stats.py'\n",
    "exp_name = 'example'\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "exp_runtime = 100\n",
    "\n",
    "# Submit a job with batch size 128 and 1 epoch for testing\n",
    "opt = JobOptions(256, 1)\n",
    "\n",
    "# Save the output to this file\n",
    "master.submit(opt, '/home/diego/experiments/test.out', 200)\n",
    "\n",
    "# Start the script to gather the cpu and io_wait usage\n",
    "for n in nodes:\n",
    "    # execute the command to get the cpu and io_wait stats\n",
    "    print(f'Executing command in {n.name}...')\n",
    "    n.ssh.exec_command(f\"\"\"python3 {script_path} -o {exp_folder}{exp_name} -m {exp_runtime} > {exp_folder}{exp_name}-data.out\"\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}