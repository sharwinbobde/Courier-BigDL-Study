{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test to run the experiments in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import paramiko\n",
    "from dataclasses import dataclass\n",
    "from libcloud.compute.providers import get_driver\n",
    "from libcloud.compute.types import Provider\n",
    "from config import config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure the SSH user (Massoud cause it's the user where the bd folder is)\n",
    "\n",
    "You need to create a GCLOUD-ACCOUNT with an extra key as explained here: https://libcloud.readthedocs.io/en/stable/compute/drivers/gce.html\n",
    "and add it to the GCLOUD-KEY-PATH for the driver. The private RSA key named PKEY you'll find in your .ssh folder,\n",
    "normally with the name gcloud-compute-platform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "SSH_USER = config['SSH_USER']  #Username that you use to connect to server through ssh\n",
    "BIGDL_USER = config[\"BIGDL_USER\"]  #Username under which BIGDL is installed on server\n",
    "GCLOUD_ACCOUNT = config[\"GCLOUD_ACCOUNT\"]\n",
    "GCLOUD_KEY_PATH = config[\"GCLOUD_KEY_PATH\"]  # The path to the Service Account Key (a JSON file)\n",
    "GCLOUD_PROJECT = config[\"GCLOUD_PROJECT\"]  # GCloud project id\n",
    "PKEY = config[\"PKEY\"]\n",
    "DESIGN_CSV = config[\"DESIGN_CSV\"]  # The CSV with the experiment design"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "ComputeEngine = get_driver(Provider.GCE)\n",
    "\n",
    "driver = ComputeEngine(GCLOUD_ACCOUNT, GCLOUD_KEY_PATH, project=GCLOUD_PROJECT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[<Node: uuid=82ed007bce59a0be16dbaf5b59be4985f3a14033, name=master, state=RUNNING, public_ips=['34.82.173.8'], private_ips=['10.138.0.14'], provider=Google Compute Engine ...>,\n <Node: uuid=8d455a8c58049047167eeae04a173d09955d0e05, name=slave-1, state=RUNNING, public_ips=['34.82.128.137'], private_ips=['10.138.0.15'], provider=Google Compute Engine ...>,\n <Node: uuid=47883987b6be947d3b9b20ddf65c975e4e162a0f, name=slave-2, state=RUNNING, public_ips=['35.233.147.220'], private_ips=['10.138.0.16'], provider=Google Compute Engine ...>,\n <Node: uuid=45346fe0cffb6b492dc3baffe5bc92e0930a6db6, name=slave-3, state=RUNNING, public_ips=['35.233.165.183'], private_ips=['10.138.0.17'], provider=Google Compute Engine ...>]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "driver.list_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the Node definitions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "\n",
    "class Node(ABC):\n",
    "    def __init__(self, driver, name, master=False, masterNode=None):\n",
    "        \"\"\"Basic Node \"\"\"\n",
    "        print(f'Starting node with name {name}')\n",
    "        self.driver = driver\n",
    "        self.name=name\n",
    "        if not master and masterNode == None:\n",
    "            raise ValueError(\"Slave nodes need a master\")\n",
    "        self.master = masterNode\n",
    "        _nodes = self.driver.list_nodes()\n",
    "        for n in _nodes:\n",
    "            if n.name == self.name:\n",
    "                print(f'Found node {n} with name {n.name} and IPs {n.public_ips}, {n.private_ips}')\n",
    "                self.public_ip = n.public_ips[0]\n",
    "                self.private_ip = n.private_ips[0]\n",
    "        self.connected = False\n",
    "\n",
    "        for i in range(5):  # Try 5 times\n",
    "            try:\n",
    "                self.open_ssh()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "            if not self.connected:\n",
    "                raise RuntimeError(f\"Can't connect to node {self.name}\")\n",
    "        self.start_type()\n",
    "\n",
    "    def open_ssh(self):\n",
    "        self.ssh = paramiko.SSHClient()\n",
    "        self.ssh.load_system_host_keys()\n",
    "        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        self.k = paramiko.RSAKey.from_private_key_file(PKEY)\n",
    "        self.ssh.connect(self.public_ip, username=SSH_USER, pkey=self.k)\n",
    "        self.connected = True\n",
    "        print(f'Node {self.name} connected via ssh')\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close_ssh()\n",
    "\n",
    "    def close_ssh(self):\n",
    "        self.connected = False\n",
    "        self.ssh.close()\n",
    "\n",
    "    @abstractmethod\n",
    "    def start_type(self):\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class JobOptions:\n",
    "    core_number: int\n",
    "    batch_size: int\n",
    "    max_epochs: int\n",
    "    network: str\n",
    "\n",
    "class MasterNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(\n",
    "            f'/home/{BIGDL_USER}/bd/spark/sbin/start-master.sh')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "    def submit(self, options: JobOptions, filename, save_path:str, timeout=None, blocking=False):\n",
    "\n",
    "\n",
    "        command = f\"\"\"/home/{BIGDL_USER}/bd/spark/bin/spark-submit --master spark://{self.private_ip}:7077 --driver-cores 4 \\\n",
    "                    --driver-memory 6G --total-executor-cores {options.core_number} --executor-cores 1 --executor-memory 3G \\\n",
    "                    --py-files /home/{BIGDL_USER}/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/{BIGDL_USER}/bd/mnist/{options.network}.py \\\n",
    "                    --properties-file /home/{BIGDL_USER}/bd/spark/conf/spark-bigdl.conf \\\n",
    "                    --jars /home/{BIGDL_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.driver.extraClassPath=/home/{BIGDL_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/{BIGDL_USER}/bd/mnist/{options.network}.py \\\n",
    "                    --action train --dataPath /tmp/mnist --batchSize {options.batch_size} --endTriggerNum {options.max_epochs} > {save_path}{filename}.log\"\"\"\n",
    "\n",
    "        print(command)\n",
    "\n",
    "\n",
    "        # Get the stdout and err out in case we want the command to run blocking\n",
    "        if not blocking:\n",
    "            self.ssh.exec_command(command)\n",
    "\n",
    "        else:\n",
    "            _, stdout, stderr = self.ssh.exec_command(command)\n",
    "\n",
    "            if len(stderr.read()) > 0:\n",
    "                print(f'There were some errors running the experiment {filename}')\n",
    "                print(stdout.read())\n",
    "                print(stderr.read())\n",
    "\n",
    "\n",
    "\n",
    "    def cancel(self):\n",
    "        br = mechanize.Browser()\n",
    "        br.open(f\"http://{self.public_ip}:8080\")\n",
    "\n",
    "        def select_form(form):\n",
    "            return form.attrs.get('action', None) == 'app/kill/'\n",
    "        try:\n",
    "            br.select_form(predicate=select_form)\n",
    "        except mechanize._mechanize.FormNotFoundError:\n",
    "            print(\"FormNotFoundError\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during cancelloing.\")\n",
    "            print(e)\n",
    "        br.submit()\n",
    "\n",
    "\n",
    "class SlaveNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f'/home/{BIGDL_USER}/bd/spark/sbin/start-slave.sh spark://{self.master.private_ip}:7077')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create the nodes (master and slaves)\n",
    "\n",
    "You just have to introduce the name of the node and it automatically finds it and starts all the\n",
    "daemons necessary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Starting node with name master\n",
      "Found node <Node: uuid=82ed007bce59a0be16dbaf5b59be4985f3a14033, name=master, state=RUNNING, public_ips=['34.82.173.8'], private_ips=['10.138.0.14'], provider=Google Compute Engine ...> with name master and IPs ['34.82.173.8'], ['10.138.0.14']\n",
      "Node master connected via ssh\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Try to connect to the master node\n",
    "master = MasterNode(driver, 'master', master=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Starting node with name slave-1\n",
      "Found node <Node: uuid=8d455a8c58049047167eeae04a173d09955d0e05, name=slave-1, state=RUNNING, public_ips=['34.82.128.137'], private_ips=['10.138.0.15'], provider=Google Compute Engine ...> with name slave-1 and IPs ['34.82.128.137'], ['10.138.0.15']\n",
      "Node slave-1 connected via ssh\n",
      "Starting node with name slave-2\n",
      "Found node <Node: uuid=47883987b6be947d3b9b20ddf65c975e4e162a0f, name=slave-2, state=RUNNING, public_ips=['35.233.147.220'], private_ips=['10.138.0.16'], provider=Google Compute Engine ...> with name slave-2 and IPs ['35.233.147.220'], ['10.138.0.16']\n",
      "Node slave-2 connected via ssh\n",
      "Starting node with name slave-3\n",
      "Found node <Node: uuid=45346fe0cffb6b492dc3baffe5bc92e0930a6db6, name=slave-3, state=RUNNING, public_ips=['35.233.165.183'], private_ips=['10.138.0.17'], provider=Google Compute Engine ...> with name slave-3 and IPs ['35.233.165.183'], ['10.138.0.17']\n",
      "Node slave-3 connected via ssh\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "# Try to create the other slaves\n",
    "s1 = SlaveNode(driver, 'slave-1', master=False, masterNode=master)\n",
    "s2 = SlaveNode(driver, 'slave-2', master=False, masterNode=master)\n",
    "s3 = SlaveNode(driver, 'slave-3', master=False, masterNode=master)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# Create a list of nodes so we can run the commands on all of them easily\n",
    "from typing import List\n",
    "nodes :List[Node]= []\n",
    "nodes.extend([master,s1,s2, s3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####Read the csv with the design and run all the experiments in a loop\n",
    "\n",
    "We need to iterate the experiments and wait for the previous ones to complete\n",
    "The format of the CSV with the experiments is\n",
    "\n",
    "exp-number, cpus, batch-size, njobs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "   Index  cpu  batch  njobs    network\n0      0    1     64      1  simplenet",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Index</th>\n      <th>cpu</th>\n      <th>batch</th>\n      <th>njobs</th>\n      <th>network</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>64</td>\n      <td>1</td>\n      <td>simplenet</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 47
    }
   ],
   "source": [
    "# declare the parameters to save the data and the logs\n",
    "exp_folder = f'/home/{SSH_USER}/experiments/'\n",
    "script_path = f'/home/{SSH_USER}/cpu_io_stats.py'\n",
    "\n",
    "# experiment designs\n",
    "factorial_2k = 'experiment_designs/2k_design.csv'\n",
    "full_fact = 'experiment_designs/fullfact.csv'\n",
    "\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "EXP_RUNTIME = 5 * 60 # 5 minutes runtime by default\n",
    "EPOCHS = 10 # 30 epochs by default\n",
    "\n",
    "\n",
    "# read the file that we're interested in\n",
    "exp = pd.read_csv(full_fact, dtype={'cpu': int, \n",
    "                                    'batch': int,\n",
    "                                    'njobs': int,\n",
    "                                    'network': str})\n",
    "exp.columns.values[0] = 'Index'\n",
    "exp.set_index('Index')\n",
    "\n",
    "exp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the experiments in a loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "\n 0-cpu1-batch64-njobs1-netsimplenet\n",
      "Submitting 1 job(s) to the master with batch 64, 1 cores and simplenet network.\nTask number 0, blocking = True\n/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.138.0.14:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 1 --executor-cores 1 --executor-memory 3G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/simplenet.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/simplenet.py                     --action train --dataPath /tmp/mnist --batchSize 64 --endTriggerNum 10 > /home/tomek/experiments/0-cpu1-batch64-njobs1-netsimplenet/0-cpu1-batch64-njobs1-netsimplenet.log\n",
      "\nExperiments finished after 7.0110540310541785 minutes\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Remove previous experiments to make room for the new (with a dialog for safety)\n",
    "response = str(input('Removing previous experiments from the servers, continue? (y/N)'))\n",
    "if response.lower() != 'y':\n",
    "    print('stopping...')\n",
    "    # quit the execution \"nicely\" without stopping the kernel\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "# Run the scripts in the multiple nddes\n",
    "for n in nodes:\n",
    "    # Create the folder for the experiment\n",
    "\n",
    "    print('Removing previous experiments...')\n",
    "    _, _, stderr = n.ssh.exec_command(f'rm -rf {exp_folder}*')\n",
    "    if len(stderr.read()) != 0:\n",
    "        print('Error removing folder', stderr.read())\n",
    "        exit(-1)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for idx, row in exp.iterrows():\n",
    "\n",
    "    # build the JobOptions with 30 epochs by default\n",
    "    job = JobOptions(core_number=row.cpu, batch_size=row.batch, max_epochs=EPOCHS, network=row.network)\n",
    "    filename = f'{int(row.Index)}-cpu{row.cpu}-batch{row.batch}-njobs{row.njobs}-net{row.network}'\n",
    "    print('\\n',filename)\n",
    "\n",
    "    for n in nodes:\n",
    "        # print('Creating folder in', n.name)\n",
    "        _, _, stderr = n.ssh.exec_command(f'mkdir {exp_folder}{filename}')\n",
    "        if len(stderr.read()) != 0:\n",
    "            print('Error creating folder', stderr.read())\n",
    "            exit(-1)\n",
    "\n",
    "\n",
    "\n",
    "        # execute the command to get the cpu and io_wait stats\n",
    "        # print(f'Executing command in {n.name}...')\n",
    "        n.ssh.exec_command(f\"\"\"python3 {script_path} \\\n",
    "        -o {exp_folder}{filename}/{filename} -m {EXP_RUNTIME} >{exp_folder}{filename}/{filename}-data.out 2>&1\"\"\")\n",
    "\n",
    "\n",
    "    njobs = row.njobs\n",
    "    # submit job to the master\n",
    "    print(f'Submitting {njobs} job(s) to the master with batch {row.batch}, {row.cpu} cores and {row.network} network.')\n",
    "\n",
    "    try:\n",
    "        for i in range(njobs):\n",
    "            # Only block after submitting all of the tasks\n",
    "            blocking = False if i < (njobs-1) else True\n",
    "            print(f'Task number {i}, blocking = {blocking}')\n",
    "            master.submit(job, save_path = f'{exp_folder}{filename}/', filename= filename, timeout=200, blocking=blocking)\n",
    "    except Exception as e:\n",
    "        print(f'Error in the command:', e)\n",
    "        master.cancel()\n",
    "\n",
    "\n",
    "print(f'\\nExperiments finished after {(time.time()-start)/60} minutes')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run the experiment and the script\n",
    "\n",
    "You need to set the account and the location of the cpu-io script so it runs.\n",
    "You also must define the number of seconds of runtime of the script and your experiment folder.\n",
    "\n",
    "the script is run with ```python3 script -o [output_file] -m [max_seconds] -i [interval]```\n",
    "\n",
    "- output file is a pickle file in the experiments folder\n",
    "- max seconds is the run_time of the script. Should be less than the training time of the experiment\n",
    "- interval at which the measurements are taken."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_folder = f'/home/{SSH_USER}/experiments/'\n",
    "script_path = f'/home/{SSH_USER}/cpu_io_stats.py'\n",
    "exp_name = 'example'\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "exp_runtime = 100\n",
    "\n",
    "# Submit a job with batch size 128 and 1 epoch for testing\n",
    "opt = JobOptions(256, 1)\n",
    "\n",
    "# Save the output to this file\n",
    "master.submit(opt, f'/home/{SSH_USER}/experiments/test.out', 200)\n",
    "\n",
    "# Start the script to gather the cpu and io_wait usage\n",
    "for n in nodes:\n",
    "    # execute the command to get the cpu and io_wait stats\n",
    "    print(f'Executing command in {n.name}...')\n",
    "    n.ssh.exec_command(f\"\"\"python3 {script_path} -o {exp_folder}{exp_name} -m {exp_runtime} > {exp_folder}{exp_name}-data.out\"\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}