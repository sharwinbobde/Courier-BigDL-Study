{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test to run the experiments in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import paramiko\n",
    "from dataclasses import dataclass\n",
    "from libcloud.compute.providers import get_driver\n",
    "from libcloud.compute.types import Provider\n",
    "from config import config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure the SSH user (Massoud cause it's the user where the bd folder is)\n",
    "\n",
    "You need to create a GCLOUD-ACCOUNT with an extra key as explained here: https://libcloud.readthedocs.io/en/stable/compute/drivers/gce.html\n",
    "and add it to the GCLOUD-KEY-PATH for the driver. The private RSA key named PKEY you'll find in your .ssh folder,\n",
    "normally with the name gcloud-compute-platform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "SSH_USER = config['SSH_USER']  #Username that you use to connect to server through ssh\n",
    "BIGDL_USER = config[\"BIGDL_USER\"]  #Username under which BIGDL is installed on server\n",
    "GCLOUD_ACCOUNT = config[\"GCLOUD_ACCOUNT\"]\n",
    "GCLOUD_KEY_PATH = config[\"GCLOUD_KEY_PATH\"]  # The path to the Service Account Key (a JSON file)\n",
    "GCLOUD_PROJECT = config[\"GCLOUD_PROJECT\"]  # GCloud project id\n",
    "PKEY = config[\"PKEY\"]\n",
    "DESIGN_CSV = config[\"DESIGN_CSV\"]  # The CSV with the experiment design"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "ComputeEngine = get_driver(Provider.GCE)\n",
    "\n",
    "driver = ComputeEngine(GCLOUD_ACCOUNT, GCLOUD_KEY_PATH, project=GCLOUD_PROJECT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "[<Node: uuid=9c7e7aaae8d3f613db481baf2a651a2f8a0a1456, name=bigdl, state=STOPPED, public_ips=[None], private_ips=['10.128.0.4'], provider=Google Compute Engine ...>,\n <Node: uuid=be87d74e1a19c10817ef5f4199d09885481fc3a9, name=master, state=RUNNING, public_ips=['34.82.126.184'], private_ips=['10.138.0.4'], provider=Google Compute Engine ...>,\n <Node: uuid=ff6522e1d2ed0bb068c66f8cee5cc91799452568, name=slave-1, state=RUNNING, public_ips=['35.247.35.147'], private_ips=['10.138.0.6'], provider=Google Compute Engine ...>,\n <Node: uuid=13cbb18a173fe709272a72e2cc261052cd8c90be, name=slave-2, state=RUNNING, public_ips=['34.83.163.23'], private_ips=['10.138.0.5'], provider=Google Compute Engine ...>]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "driver.list_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the Node definitions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Node(ABC):\n",
    "    def __init__(self, driver, name, master=False, masterNode=None):\n",
    "        \"\"\"Basic Node \"\"\"\n",
    "        print(f'Starting node with name {name}')\n",
    "        self.driver = driver\n",
    "        self.name=name\n",
    "        if not master and masterNode == None:\n",
    "            raise ValueError(\"Slave nodes need a master\")\n",
    "        self.master = masterNode\n",
    "        _nodes = self.driver.list_nodes()\n",
    "        for n in _nodes:\n",
    "            if n.name == self.name:\n",
    "                print(f'Found node {n} with name {n.name} and IPs {n.public_ips}, {n.private_ips}')\n",
    "                self.public_ip = n.public_ips[0]\n",
    "                self.private_ip = n.private_ips[0]\n",
    "        self.connected = False\n",
    "        self.running = False\n",
    "\n",
    "        for i in range(5):  # Try 5 times\n",
    "            try:\n",
    "                self.open_ssh()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "            if not self.connected:\n",
    "                raise RuntimeError(f\"Can't connect to node {self.name}\")\n",
    "        self.start_type()\n",
    "\n",
    "    def open_ssh(self):\n",
    "        self.ssh = paramiko.SSHClient()\n",
    "        self.ssh.load_system_host_keys()\n",
    "        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        self.k = paramiko.RSAKey.from_private_key_file(PKEY)\n",
    "        self.ssh.connect(self.public_ip, username=SSH_USER, pkey=self.k)\n",
    "        self.connected = True\n",
    "        print(f'Node {self.name} connected via ssh')\n",
    "\n",
    "    def reconnect_ssh(self):\n",
    "        \"\"\" After a long time maybe the ssh closes,\n",
    "        we need to restart the connection \"\"\"\n",
    "        self.ssh.connect(self.public_ip, username=USER, pkey=self.k)\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close_ssh()\n",
    "\n",
    "    def close_ssh(self):\n",
    "        self.connected = False\n",
    "        self.ssh.close()\n",
    "\n",
    "    @abstractmethod\n",
    "    def start_type(self):\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class JobOptions:\n",
    "    core_number: int\n",
    "    batch_size: int\n",
    "    max_epochs: int\n",
    "\n",
    "class MasterNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(\n",
    "            f'/home/{BIGDL_USER}/bd/spark/sbin/start-master.sh')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "    def submit(self, options: JobOptions, filename, save_path:str, timeout=None, blocking=False):\n",
    "\n",
    "        # # thread to keep the ssh alive\n",
    "        # keep_alive_thread = threading.Thread(target=self._keep_ssh_alive())\n",
    "        #\n",
    "        # self.running = True\n",
    "        # keep_alive_thread.start()\n",
    "\n",
    "        command = f\"\"\"/home/{BIGDL_USER}/bd/spark/bin/spark-submit --master spark://{self.private_ip}:7077 --driver-cores 4 \\\n",
    "                    --driver-memory 6G --total-executor-cores {options.core_number} --executor-cores 1 --executor-memory 3G \\\n",
    "                    --py-files /home/{BIGDL_USER}/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/{BIGDL_USER}/bd/mnist/lenet5.py \\\n",
    "                    --properties-file /home/{BIGDL_USER}/bd/spark/conf/spark-bigdl.conf \\\n",
    "                    --jars /home/{BIGDL_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.driver.extraClassPath=/home/{BIGDL_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/{BIGDL_USER}/bd/mnist/lenet5.py \\\n",
    "                    --action train --dataPath /tmp/mnist --batchSize {options.batch_size} --endTriggerNum {options.max_epochs} > {save_path}{filename}.log\"\"\"\n",
    "\n",
    "        print(command)\n",
    "\n",
    "\n",
    "        # Get the stdout and err out in case we want the command to run blocking\n",
    "        if not blocking:\n",
    "            self.ssh.exec_command(command)\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            self.ssh.exec_command(command)\n",
    "\n",
    "\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                time.sleep(10)\n",
    "                _, out, _ = self.ssh.exec_command('ps aux | grep spark-submit')\n",
    "                proc = out.read().decode('utf-8')\n",
    "                num_procs = len(proc.split('\\n'))\n",
    "\n",
    "                print(f'There are {num_procs} processes returned')\n",
    "                if num_procs < 4:\n",
    "                    print('Process is finished, exiting...')\n",
    "                    finished = True\n",
    "\n",
    "\n",
    "        # self.running = False\n",
    "\n",
    "        print('Master exiting...')\n",
    "\n",
    "\n",
    "\n",
    "    def cancel(self):\n",
    "        br = mechanize.Browser()\n",
    "        br.open(f\"http://{self.public_ip}:8080\")\n",
    "\n",
    "        def select_form(form):\n",
    "            return form.attrs.get('action', None) == 'app/kill/'\n",
    "        try:\n",
    "            br.select_form(predicate=select_form)\n",
    "        except mechanize._mechanize.FormNotFoundError:\n",
    "            print(\"FormNotFoundError\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during cancelloing.\")\n",
    "            print(e)\n",
    "        br.submit()\n",
    "\n",
    "    def _keep_ssh_alive(self):\n",
    "        while self.running:\n",
    "            # send a command every 60 seconds\n",
    "            print('Sending command to the server...')\n",
    "            self.ssh.exec_command('ls')\n",
    "            time.sleep(60)\n",
    "\n",
    "\n",
    "\n",
    "class SlaveNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f'/home/{BIGDL_USER}/bd/spark/sbin/start-slave.sh spark://{self.master.private_ip}:7077')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create the nodes (master and slaves)\n",
    "\n",
    "You just have to introduce the name of the node and it automatically finds it and starts all the\n",
    "daemons necessary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Starting node with name bigdl-master-1\n",
      "Found node <Node: uuid=3124f0c0c1eaf12f9aeef33c7d532c7952535b20, name=bigdl-master-1, state=RUNNING, public_ips=['34.67.204.85'], private_ips=['10.128.0.11'], provider=Google Compute Engine ...> with name bigdl-master-1 and IPs ['34.67.204.85'], ['10.128.0.11']\n",
      "Node bigdl-master-1 connected via ssh\n"
     ]
    }
   ],
   "source": [
    "# Try to connect to the master node\n",
    "\n",
    "master = MasterNode(driver, 'bigdl-master-1', master=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting node with name slave-1\n",
      "Found node <Node: uuid=68c98a4af0f1a83c44031a02bdad5503b94447b3, name=slave-1, state=RUNNING, public_ips=['35.239.159.190'], private_ips=['10.128.0.15'], provider=Google Compute Engine ...> with name slave-1 and IPs ['35.239.159.190'], ['10.128.0.15']\n",
      "Node slave-1 connected via ssh\n",
      "Starting node with name slave-2\n",
      "Found node <Node: uuid=412cc4e5e482b0e7ff44f0c276b07adec8c6aa20, name=slave-2, state=RUNNING, public_ips=['34.123.251.206'], private_ips=['10.128.0.16'], provider=Google Compute Engine ...> with name slave-2 and IPs ['34.123.251.206'], ['10.128.0.16']\n",
      "Node slave-2 connected via ssh\n",
      "Starting node with name slave-3\n",
      "Found node <Node: uuid=587a4159388cbe73941e162834b68aae06c9af12, name=slave-3, state=RUNNING, public_ips=['35.224.46.25'], private_ips=['10.128.0.17'], provider=Google Compute Engine ...> with name slave-3 and IPs ['35.224.46.25'], ['10.128.0.17']\n",
      "Node slave-3 connected via ssh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try to create the other slaves\n",
    "s1 = SlaveNode(driver, 'slave-1', master=False, masterNode=master)\n",
    "s2 = SlaveNode(driver, 'slave-2', master=False, masterNode=master)\n",
    "s3 = SlaveNode(driver, 'slave-3', master=False, masterNode=master)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [
    "# Create a list of nodes so we can run the commands on all of them easily\n",
    "from typing import List\n",
    "nodes :List[Node]= []\n",
    "nodes.extend([master,s1,s2,s3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Read the csv with the design and run all the experiments in a loop\n",
    "\n",
    "We need to iterate the experiments and wait for the previous ones to complete\n",
    "The format of the CSV with the experiments is\n",
    "\n",
    "exp-number, cpus, batch-size, njobs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "data": {
      "text/plain": "   Index  cpu  batch  njobs\n2      2    1    512      1\n3      3    8    512      1\n6      6    1    512      5\n7      7    8    512      5\n0      0    1     64      1\n1      1    8     64      1\n4      4    1     64      5\n5      5    8     64      5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Index</th>\n      <th>cpu</th>\n      <th>batch</th>\n      <th>njobs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n      <td>512</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>8</td>\n      <td>512</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>1</td>\n      <td>512</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>8</td>\n      <td>512</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>64</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>8</td>\n      <td>64</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n      <td>64</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>8</td>\n      <td>64</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare the parameters to save the data and the logs\n",
    "exp_folder = f'/home/{SSH_USER}/experiments/'\n",
    "script_path = f'/home/{SSH_USER}/cpu_io_stats.py'\n",
    "\n",
    "# experiment designs\n",
    "factorial_2k = 'experiment_designs/2k_design.csv'\n",
    "full_fact = 'experiment_designs/fullfact.csv'\n",
    "\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "EXP_RUNTIME = 300 # 5 minutes runtime by default\n",
    "EPOCHS = 5 # 30 epochs by default\n",
    "\n",
    "\n",
    "# read the file that we're interested in\n",
    "exp = pd.read_csv(factorial_2k, dtype=int)\n",
    "exp.columns.values[0] = 'Index'\n",
    "exp.set_index('Index')\n",
    "\n",
    "exp = exp.sort_values(by='batch', ascending=False)\n",
    "exp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the experiments in a loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "Removing previous experiments...\n",
      "\n",
      " 2-cpu1-batch512-njobs1\n",
      "Creating folder in bigdl-master-1\n",
      "Executing command in bigdl-master-1...\n",
      "Creating folder in slave-1\n",
      "Executing command in slave-1...\n",
      "Creating folder in slave-2\n",
      "Executing command in slave-2...\n",
      "Creating folder in slave-3\n",
      "Executing command in slave-3...\n",
      "Submitting 1 job(s) to the master with batch 512 and 1 cores\n",
      "Task number 0, blocking = True\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 1 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/2-cpu1-batch512-njobs1/2-cpu1-batch512-njobs1.log\n",
      "There are 4 processes returned\n",
      "There are 3 processes returned\n",
      "Process is finished, exiting...\n",
      "Master exiting...\n",
      "\n",
      " 3-cpu8-batch512-njobs1\n",
      "Creating folder in bigdl-master-1\n",
      "Executing command in bigdl-master-1...\n",
      "Creating folder in slave-1\n",
      "Executing command in slave-1...\n",
      "Creating folder in slave-2\n",
      "Executing command in slave-2...\n",
      "Creating folder in slave-3\n",
      "Executing command in slave-3...\n",
      "Submitting 1 job(s) to the master with batch 512 and 8 cores\n",
      "Task number 0, blocking = True\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/3-cpu8-batch512-njobs1/3-cpu8-batch512-njobs1.log\n",
      "There are 4 processes returned\n",
      "There are 3 processes returned\n",
      "Process is finished, exiting...\n",
      "Master exiting...\n",
      "\n",
      " 6-cpu1-batch512-njobs5\n",
      "Creating folder in bigdl-master-1\n",
      "Executing command in bigdl-master-1...\n",
      "Creating folder in slave-1\n",
      "Executing command in slave-1...\n",
      "Creating folder in slave-2\n",
      "Executing command in slave-2...\n",
      "Creating folder in slave-3\n",
      "Executing command in slave-3...\n",
      "Submitting 5 job(s) to the master with batch 512 and 1 cores\n",
      "Task number 0, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 1 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/6-cpu1-batch512-njobs5/6-cpu1-batch512-njobs5.log\n",
      "Task number 1, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 1 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/6-cpu1-batch512-njobs5/6-cpu1-batch512-njobs5.log\n",
      "Task number 2, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 1 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/6-cpu1-batch512-njobs5/6-cpu1-batch512-njobs5.log\n",
      "Task number 3, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 1 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/6-cpu1-batch512-njobs5/6-cpu1-batch512-njobs5.log\n",
      "Task number 4, blocking = True\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 1 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/6-cpu1-batch512-njobs5/6-cpu1-batch512-njobs5.log\n",
      "There are 8 processes returned\n",
      "There are 4 processes returned\n",
      "There are 4 processes returned\n",
      "There are 4 processes returned\n",
      "There are 4 processes returned\n",
      "There are 4 processes returned\n",
      "There are 4 processes returned\n",
      "There are 4 processes returned\n",
      "There are 4 processes returned\n",
      "There are 4 processes returned\n",
      "There are 4 processes returned\n",
      "There are 3 processes returned\n",
      "Process is finished, exiting...\n",
      "Master exiting...\n",
      "\n",
      " 7-cpu8-batch512-njobs5\n",
      "Creating folder in bigdl-master-1\n",
      "Executing command in bigdl-master-1...\n",
      "Creating folder in slave-1\n",
      "Executing command in slave-1...\n",
      "Creating folder in slave-2\n",
      "Executing command in slave-2...\n",
      "Creating folder in slave-3\n",
      "Executing command in slave-3...\n",
      "Submitting 5 job(s) to the master with batch 512 and 8 cores\n",
      "Task number 0, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/7-cpu8-batch512-njobs5/7-cpu8-batch512-njobs5.log\n",
      "Task number 1, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/7-cpu8-batch512-njobs5/7-cpu8-batch512-njobs5.log\n",
      "Task number 2, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/7-cpu8-batch512-njobs5/7-cpu8-batch512-njobs5.log\n",
      "Task number 3, blocking = False\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/7-cpu8-batch512-njobs5/7-cpu8-batch512-njobs5.log\n",
      "Task number 4, blocking = True\n",
      "/home/am72ghiassi/bd/spark/bin/spark-submit --master spark://10.128.0.11:7077 --driver-cores 4                     --driver-memory 6G --total-executor-cores 8 --executor-cores 1 --executor-memory 10G                     --py-files /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/am72ghiassi/bd/mnist/lenet5.py                     --properties-file /home/am72ghiassi/bd/spark/conf/spark-bigdl.conf                     --jars /home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.driver.extraClassPath=/home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar                     --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/am72ghiassi/bd/mnist/lenet5.py                     --action train --dataPath /tmp/mnist --batchSize 512 --endTriggerNum 5 > /home/diego/experiments/7-cpu8-batch512-njobs5/7-cpu8-batch512-njobs5.log\n",
      "There are 8 processes returned\n"
     ]
    }
   ],
   "source": [
    "# Remove previous experiments to make room for the new (with a dialog for safety)\n",
    "response = str(input('Removing previous experiments from the servers, continue? (y/N)'))\n",
    "if response.lower() != 'y':\n",
    "    print('stopping...')\n",
    "    # quit the execution \"nicely\" without stopping the kernel\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "# Run the scripts in the multiple nddes\n",
    "for n in nodes:\n",
    "    # Create the folder for the experiment\n",
    "\n",
    "    print('Removing previous experiments...')\n",
    "    _, _, stderr = n.ssh.exec_command(f'rm -rf {exp_folder}*')\n",
    "    if len(stderr.read()) != 0:\n",
    "        print('Error creating folder', stderr.read())\n",
    "        exit(-1)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for idx, row in exp.iterrows():\n",
    "\n",
    "    # build the JobOptions with 30 epochs by default\n",
    "    job = JobOptions(core_number=row.cpu, batch_size=row.batch, max_epochs=EPOCHS)\n",
    "    filename = f'{int(row.Index)}-cpu{row.cpu}-batch{row.batch}-njobs{row.njobs}'\n",
    "    print('\\n',filename)\n",
    "\n",
    "    for n in nodes:\n",
    "        print('Creating folder in', n.name)\n",
    "\n",
    "        # # Try random command and reopen the ssh shell if needed\n",
    "        try:\n",
    "            _, _, stderr = n.ssh.exec_command('ls')\n",
    "        except Exception as e:\n",
    "            print('Issue while connecting to node, reconnecting...')\n",
    "            n.reconnect_ssh()\n",
    "\n",
    "        _, _, stderr = n.ssh.exec_command(f'mkdir {exp_folder}{filename}')\n",
    "        if len(stderr.read()) != 0:\n",
    "            print('Error creating folder', stderr.read())\n",
    "            exit(-1)\n",
    "\n",
    "\n",
    "\n",
    "        # execute the command to get the cpu and io_wait stats\n",
    "        print(f'Executing command in {n.name}...')\n",
    "        n.ssh.exec_command(f\"\"\"python3 {script_path} \\\n",
    "        -o {exp_folder}{filename}/{filename} -m {EXP_RUNTIME} >{exp_folder}{filename}/{filename}-data.out 2>&1\"\"\")\n",
    "\n",
    "\n",
    "    njobs = row.njobs\n",
    "    # submit job to the master\n",
    "    print(f'Submitting {njobs} job(s) to the master with batch {row.batch} and {row.cpu} cores')\n",
    "\n",
    "    try:\n",
    "        for i in range(njobs):\n",
    "            # Only block after submitting all of the tasks\n",
    "            blocking = False if i < (njobs-1) else True\n",
    "            print(f'Task number {i}, blocking = {blocking}')\n",
    "            master.submit(job, save_path = f'{exp_folder}{filename}/', filename= filename, timeout=200, blocking=blocking)\n",
    "    except Exception as e:\n",
    "        print(f'Error in the command:', e)\n",
    "        master.cancel()\n",
    "\n",
    "\n",
    "print(f'\\nExperiments finished after {(time.time()-start)/60} minutes')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run the experiment and the script\n",
    "\n",
    "You need to set the account and the location of the cpu-io script so it runs.\n",
    "You also must define the number of seconds of runtime of the script and your experiment folder.\n",
    "\n",
    "the script is run with ```python3 script -o [output_file] -m [max_seconds] -i [interval]```\n",
    "\n",
    "- output file is a pickle file in the experiments folder\n",
    "- max seconds is the run_time of the script. Should be less than the training time of the experiment\n",
    "- interval at which the measurements are taken.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_folder = f'/home/{SSH_USER}/experiments/'\n",
    "script_path = f'/home/{SSH_USER}/cpu_io_stats.py'\n",
    "exp_name = 'example'\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "exp_runtime = 100\n",
    "\n",
    "# Submit a job with batch size 128 and 1 epoch for testing\n",
    "opt = JobOptions(256, 1)\n",
    "\n",
    "# Save the output to this file\n",
    "master.submit(opt, f'/home/{SSH_USER}/experiments/test.out', 200)\n",
    "\n",
    "# Start the script to gather the cpu and io_wait usage\n",
    "for n in nodes:\n",
    "    # execute the command to get the cpu and io_wait stats\n",
    "    print(f'Executing command in {n.name}...')\n",
    "    n.ssh.exec_command(f\"\"\"python3 {script_path} -o {exp_folder}{exp_name} -m {exp_runtime} > {exp_folder}{exp_name}-data.out\"\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}