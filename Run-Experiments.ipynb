{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test to run the experiments in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import paramiko\n",
    "from dataclasses import dataclass\n",
    "from libcloud.compute.providers import get_driver\n",
    "from libcloud.compute.types import Provider\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configure the SSH user \n",
    "\n",
    "You need to create a GCLOUD-ACCOUNT with an extra key as explained here: https://libcloud.readthedocs.io/en/stable/compute/drivers/gce.html\n",
    "and add it to the GCLOUD-KEY-PATH for the driver. The private RSA key named PKEY you'll find in your .ssh folder,\n",
    "normally with the name gcloud-compute-platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SSH_USER = config['SSH_USER']  #Username that you use to connect to server through ssh\n",
    "BIGDL_USER = config[\"BIGDL_USER\"]  #Username under which BIGDL is installed on server\n",
    "GCLOUD_ACCOUNT = config[\"GCLOUD_ACCOUNT\"]\n",
    "GCLOUD_KEY_PATH = config[\"GCLOUD_KEY_PATH\"]  # The path to the Service Account Key (a JSON file)\n",
    "GCLOUD_PROJECT = config[\"GCLOUD_PROJECT\"]  # GCloud project id\n",
    "PKEY = config[\"PKEY\"]\n",
    "DESIGN_CSV = config[\"DESIGN_CSV\"]  # The CSV with the experiment design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ComputeEngine = get_driver(Provider.GCE)\n",
    "\n",
    "driver = ComputeEngine(GCLOUD_ACCOUNT, GCLOUD_KEY_PATH, project=GCLOUD_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "driver.list_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create the Node definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Node(ABC):\n",
    "    def __init__(self, driver, name, master=False, masterNode=None):\n",
    "        \"\"\"Basic Node \"\"\"\n",
    "        print(f'Starting node with name {name}')\n",
    "        self.driver = driver\n",
    "        self.name=name\n",
    "        if not master and masterNode == None:\n",
    "            raise ValueError(\"Slave nodes need a master\")\n",
    "        self.master = masterNode\n",
    "        _nodes = self.driver.list_nodes()\n",
    "        for n in _nodes:\n",
    "            if n.name == self.name:\n",
    "                print(f'Found node {n} with name {n.name} and IPs {n.public_ips}, {n.private_ips}')\n",
    "                self.public_ip = n.public_ips[0]\n",
    "                self.private_ip = n.private_ips[0]\n",
    "        self.connected = False\n",
    "        self.running = False\n",
    "\n",
    "        for i in range(5):  # Try 5 times\n",
    "            try:\n",
    "                self.open_ssh()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "            if not self.connected:\n",
    "                raise RuntimeError(f\"Can't connect to node {self.name}\")\n",
    "        self.start_type()\n",
    "\n",
    "    def open_ssh(self):\n",
    "        self.ssh = paramiko.SSHClient()\n",
    "        self.ssh.load_system_host_keys()\n",
    "        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        self.k = paramiko.RSAKey.from_private_key_file(PKEY)\n",
    "        self.ssh.connect(self.public_ip, username=SSH_USER, pkey=self.k)\n",
    "        self.connected = True\n",
    "        print(f'Node {self.name} connected via ssh')\n",
    "\n",
    "    def reconnect_ssh(self):\n",
    "        \"\"\" After a long time maybe the ssh closes,\n",
    "        we need to restart the connection \"\"\"\n",
    "        self.ssh.connect(self.public_ip, username=SSH_USER, pkey=self.k)\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close_ssh()\n",
    "\n",
    "    def close_ssh(self):\n",
    "        self.connected = False\n",
    "        self.ssh.close()\n",
    "\n",
    "    @abstractmethod\n",
    "    def start_type(self):\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class JobOptions:\n",
    "    core_number: int\n",
    "    batch_size: int\n",
    "    max_epochs: int\n",
    "    network: str\n",
    "\n",
    "class MasterNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(\n",
    "            f'/home/{BIGDL_USER}/bd/spark/sbin/start-master.sh')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "    def submit(self, options: JobOptions, filename, save_path:str, timeout=None, blocking=False):\n",
    "\n",
    "        # # thread to keep the ssh alive\n",
    "        # keep_alive_thread = threading.Thread(target=self._keep_ssh_alive())\n",
    "        #\n",
    "        # self.running = True\n",
    "        # keep_alive_thread.start()\n",
    "\n",
    "        command = f\"\"\"/home/{BIGDL_USER}/bd/spark/bin/spark-submit --master spark://{self.private_ip}:7077 --driver-cores 4 \\\n",
    "                    --driver-memory 6G --total-executor-cores {options.core_number} --executor-cores 1 --executor-memory 3G \\\n",
    "                    --py-files /home/{BIGDL_USER}/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/{BIGDL_USER}/bd/mnist/{options.network}.py \\\n",
    "                    --properties-file /home/{BIGDL_USER}/bd/spark/conf/spark-bigdl.conf \\\n",
    "                    --jars /home/{BIGDL_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.driver.extraClassPath=/home/{BIGDL_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \\\n",
    "                    --conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/{BIGDL_USER}/bd/mnist/{options.network}.py \\\n",
    "                    --action train --dataPath /tmp/mnist --batchSize {options.batch_size} --endTriggerNum {options.max_epochs} > {save_path}{filename}.log\"\"\"\n",
    "\n",
    "#         print(command)\n",
    "\n",
    "\n",
    "        # Get the stdout and err out in case we want the command to run blocking\n",
    "        if not blocking:\n",
    "            self.ssh.exec_command(command)\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            self.ssh.exec_command(command)\n",
    "\n",
    "            # Check periodically whether the task is still running\n",
    "            # and keep alive the ssh client as well\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                time.sleep(10)\n",
    "                _, out, _ = self.ssh.exec_command('ps aux | grep spark-submit')\n",
    "                proc = out.read().decode('utf-8')\n",
    "                num_procs = len(proc.split('\\n'))\n",
    "\n",
    "                # By default this will return 3 lines:\n",
    "                # - A line with the bash command\n",
    "                # - A line with the grep\n",
    "                # - A blank line\n",
    "                #\n",
    "                # If we get more than 3 we know that spark-submit is running\n",
    "\n",
    "                # print(f'There are {num_procs} processes returned')\n",
    "                if num_procs < 4:\n",
    "                    print('Process is finished, exiting...')\n",
    "                    finished = True\n",
    "\n",
    "\n",
    "        # self.running = False\n",
    "\n",
    "        print('Master exiting...')\n",
    "\n",
    "\n",
    "\n",
    "    def cancel(self):\n",
    "        br = mechanize.Browser()\n",
    "        br.open(f\"http://{self.public_ip}:8080\")\n",
    "\n",
    "        def select_form(form):\n",
    "            return form.attrs.get('action', None) == 'app/kill/'\n",
    "        try:\n",
    "            br.select_form(predicate=select_form)\n",
    "        except mechanize._mechanize.FormNotFoundError:\n",
    "            print(\"FormNotFoundError\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during cancelloing.\")\n",
    "            print(e)\n",
    "        br.submit()\n",
    "\n",
    "    # Renew the ssh connect in case it exists\n",
    "    def _keep_ssh_alive(self):\n",
    "        while self.running:\n",
    "            # send a command every 60 seconds\n",
    "            print('Sending command to the server...')\n",
    "            self.ssh.exec_command('ls')\n",
    "            time.sleep(60)\n",
    "\n",
    "\n",
    "\n",
    "class SlaveNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f'/home/{BIGDL_USER}/bd/spark/sbin/start-slave.sh spark://{self.master.private_ip}:7077')\n",
    "        if len(stderr.read()) > 0:\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create the nodes (master and slaves)\n",
    "\n",
    "You just have to introduce the name of the node and it automatically finds it and starts all the\n",
    "daemons necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Try to connect to the master node\n",
    "master = MasterNode(driver, 'master', master=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Try to create the other slaves\n",
    "s1 = SlaveNode(driver, 'slave-1', master=False, masterNode=master)\n",
    "s2 = SlaveNode(driver, 'slave-2', master=False, masterNode=master)\n",
    "s3 = SlaveNode(driver, 'slave--3', master=False, masterNode=master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of nodes so we can run the commands on all of them easily\n",
    "from typing import List\n",
    "nodes :List[Node]= []\n",
    "nodes.extend([master,s1,s2, s3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read the csv with the design and run all the experiments in a loop\n",
    "\n",
    "We need to iterate the experiments and wait for the previous ones to complete\n",
    "The format of the CSV with the experiments is\n",
    "\n",
    "exp-number, cpus, batch-size, njobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# declare the parameters to save the data and the logs\n",
    "exp_folder = f'/home/{SSH_USER}/experiments/'\n",
    "script_path = f'/home/{SSH_USER}/cpu_io_stats.py'\n",
    "\n",
    "# experiment designs\n",
    "factorial_2k = 'experiment_designs/2k_design.csv'\n",
    "full_fact = 'experiment_designs/fullfact.csv'\n",
    "\n",
    "\n",
    "# Runtime of the CPU and IO capturing script in seconds\n",
    "EXP_RUNTIME = 5 * 60 # 5 minutes runtime by default\n",
    "EPOCHS = 10 # 10 epochs by default\n",
    "\n",
    "\n",
    "# read the file that we're interested in\n",
    "exp = pd.read_csv(full_fact, dtype={'cpu': int, \n",
    "                                    'batch': int,\n",
    "                                    'njobs': int,\n",
    "                                    'network': str})\n",
    "exp.columns.values[0] = 'Index'\n",
    "exp.set_index('Index')\n",
    "\n",
    "\n",
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run the experiments in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove previous experiments to make room for the new (with a dialog for safety)\n",
    "response = str(input('Removing previous experiments from the servers, continue? (y/N)'))\n",
    "if response.lower() != 'y':\n",
    "    print('stopping...')\n",
    "    # quit the execution \"nicely\" without stopping the kernel\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "# Run the scripts in the multiple nddes\n",
    "for n in nodes:\n",
    "    # Create the folder for the experiment\n",
    "\n",
    "    print('Removing previous experiments...')\n",
    "    _, _, stderr = n.ssh.exec_command(f'rm -rf {exp_folder}*')\n",
    "    if len(stderr.read()) != 0:\n",
    "        print('Error creating folder', stderr.read())\n",
    "        exit(-1)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for idx, row in exp.iterrows():\n",
    "\n",
    "    # build the JobOptions with the parameters from the dataframe line\n",
    "    job = JobOptions(core_number=row.cpu, batch_size=row.batch, max_epochs=EPOCHS, network=row.network)\n",
    "    filename = f'{int(row.Index)}-cpu{row.cpu}-batch{row.batch}-njobs{row.njobs}-net{row.network}'\n",
    "    print('\\n',filename)\n",
    "\n",
    "    for n in nodes:\n",
    "#         print('Creating folder in', n.name)\n",
    "\n",
    "        # Try random command and reopen the ssh shell if needed\n",
    "        try:\n",
    "            _, _, stderr = n.ssh.exec_command('ls')\n",
    "        except Exception as e:\n",
    "            print('Issue while connecting to node, reconnecting...')\n",
    "            n.reconnect_ssh()\n",
    "\n",
    "        _, _, stderr = n.ssh.exec_command(f'mkdir {exp_folder}{filename}')\n",
    "        if len(stderr.read()) != 0:\n",
    "            print('Error creating folder', stderr.read())\n",
    "            exit(-1)\n",
    "\n",
    "\n",
    "\n",
    "        # execute the command to get the cpu and io_wait stats\n",
    "#         print(f'Executing command in {n.name}...')\n",
    "        n.ssh.exec_command(f\"\"\"python3 {script_path} \\\n",
    "        -o {exp_folder}{filename}/{filename} -m {EXP_RUNTIME} >{exp_folder}{filename}/{filename}-data.out 2>&1\"\"\")\n",
    "\n",
    "\n",
    "    njobs = row.njobs\n",
    "    # submit job to the master\n",
    "    print(f'Submitting {njobs} job(s) to the master with batch {row.batch} and {row.cpu} cores')\n",
    "\n",
    "    try:\n",
    "        for i in range(njobs):\n",
    "            # Only block after submitting all of the tasks\n",
    "            blocking = False if i < (njobs-1) else True\n",
    "            print(f'Task number {i}, blocking = {blocking}')\n",
    "            master.submit(job, save_path = f'{exp_folder}{filename}/', filename= filename, timeout=200, blocking=blocking)\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f'Error in the command:', e)\n",
    "        master.cancel()\n",
    "\n",
    "\n",
    "print(f'\\nExperiments finished after {(time.time()-start)/60} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run Courier with a series of jobs\n",
    "\n",
    "\n",
    "Run Courier with a series of jobs and measure the performance of the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from courier import Job, Courier\n",
    "import pickle\n",
    "\n",
    "#load courier from pickle file\n",
    "with open('./courier.pkl', 'rb') as c:\n",
    "    courier= pickle.load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = Job(cpu=3, njobs=3, network=-1)\n",
    "b, pred = courier.optimize(j, latency=200)\n",
    "\n",
    "print('Recommended batch:', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the experiments and loop through them\n",
    "\n",
    "In one of the times we will use Courier to estimate batch and predict the runtime,\n",
    "in the other one, we will choose a random batch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = pd.read_csv('./experiment_designs/courier_exp.csv')[['cpu', 'njobs', 'network', 'latency']]\n",
    "exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_exp_start = time.time()\n",
    "\n",
    "# Remove previous experiments to make room for the new (with a dialog for safety)\n",
    "response = str(input('Removing previous experiments from the servers, continue? (y/N)'))\n",
    "if response.lower() != 'y':\n",
    "    print('stopping...')\n",
    "    # quit the execution \"nicely\" without stopping the kernel\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "# Run the scripts in the multiple nddes\n",
    "for n in nodes:\n",
    "    # Create the folder for the experiment\n",
    "\n",
    "    print('Removing previous experiments...')\n",
    "    _, _, stderr = n.ssh.exec_command(f'rm -rf {exp_folder}*')\n",
    "    if len(stderr.read()) != 0:\n",
    "        print('Error creating folder', stderr.read())\n",
    "        exit(-1)\n",
    "\n",
    "for i, row in exps.iterrows():\n",
    "    \n",
    "    # Creaee the job for the row\n",
    "    \n",
    "    j = Job(cpu=row.cpu, njobs=row.njobs, network= 1 if row.network == 'lenet5' else -1)\n",
    "    print('\\n\\n\\nStarting optimization for job ',j)\n",
    "    best_batch, (acc, t) = courier.optimize(j, latency=row.latency)\n",
    "    print(f'Chosen batch {best_batch} and predictions ({acc}, {t}), MAX_LATENCY was {row.latency}')\n",
    "    print('')\n",
    "    \n",
    "    filename = f'{int(i)}-cpu{row.cpu}-batch{best_batch}-njobs{row.njobs}-net{row.network}-courier'\n",
    "    print(filename)\n",
    "    \n",
    "    for n in nodes:\n",
    "#         print('Creating folder in', n.name)\n",
    "\n",
    "        # Try random command and reopen the ssh shell if needed\n",
    "        try:\n",
    "            _, _, stderr = n.ssh.exec_command('ls')\n",
    "        except Exception as e:\n",
    "            print('Issue while connecting to node, reconnecting...')\n",
    "            n.reconnect_ssh()\n",
    "\n",
    "        _, _, stderr = n.ssh.exec_command(f'mkdir {exp_folder}{filename}')\n",
    "        if len(stderr.read()) != 0:\n",
    "            print('Error creating folder', stderr.read())\n",
    "            exit(-1)\n",
    "    \n",
    "    # Tranform it into a JobOptions object\n",
    "    options = JobOptions(core_number=row.cpu, batch_size=best_batch, max_epochs=EPOCHS,network=row.network)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'Submitting {row.njobs} job(s) to the master with batch {best_batch} and {row.cpu} cores')\n",
    "\n",
    "    try:\n",
    "        for i in range(row.njobs):\n",
    "            # Only block after submitting all of the tasks\n",
    "            blocking = False if i < (row.njobs-1) else True\n",
    "            print(f'Task number {i}, blocking = {blocking}')\n",
    "            master.submit(options, save_path = f'{exp_folder}{filename}/', filename= filename, timeout=200, blocking=blocking)\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f'Error in the command:', e)\n",
    "        master.cancel()\n",
    "\n",
    " \n",
    "print(f'Finished the execution after {(time.time()-total_exp_start)/60} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the same but with random batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_exp_start = time.time()\n",
    "\n",
    "batches = [64,128,256, 512]\n",
    "\n",
    "for i, row in exps.iterrows():\n",
    "    \n",
    "    # Creaee the job for the row\n",
    "    best_batch = np.random.choice(batches)\n",
    "    print()\n",
    "    \n",
    "    # Tranform it into a JobOptions object\n",
    "    options = JobOptions(core_number=row.cpu, batch_size=best_batch, max_epochs=EPOCHS,network=row.network)\n",
    "    \n",
    "    print(f'Submitting {row.njobs} job(s) to the master with batch {best_batch} and {row.cpu} cores')\n",
    "    \n",
    "    filename = f'{int(i)}-cpu{row.cpu}-batch{best_batch}-njobs{row.njobs}-net{row.network}-random'\n",
    "    print(filename)\n",
    "    \n",
    "    for n in nodes:\n",
    "\n",
    "    # Try random command and reopen the ssh shell if needed\n",
    "        try:\n",
    "            _, _, stderr = n.ssh.exec_command('ls')\n",
    "        except Exception as e:\n",
    "            print('Issue while connecting to node, reconnecting...')\n",
    "            n.reconnect_ssh()\n",
    "\n",
    "        _, _, stderr = n.ssh.exec_command(f'mkdir {exp_folder}{filename}')\n",
    "        if len(stderr.read()) != 0:\n",
    "            print('Error creating folder', stderr.read())\n",
    "            exit(-1)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        for i in range(row.njobs):\n",
    "            # Only block after submitting all of the tasks\n",
    "            blocking = False if i < (row.njobs-1) else True\n",
    "            print(f'Task number {i}, blocking = {blocking}')\n",
    "            master.submit(options, save_path = f'{exp_folder}{filename}/', filename= filename, timeout=200, blocking=blocking)\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f'Error in the command:', e)\n",
    "        master.cancel()\n",
    "\n",
    " \n",
    "print(f'Finished the execution after {(time.time()-total_exp_start)/60} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
