{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Interpret the results obtained from each of the experiments\n",
    "\n",
    "The results from the experiments involve interpreting and plotting several quantities\n",
    "- CPU Utilization of the slaves during the experiments\n",
    "- IO Wait of the slaves during the experiments\n",
    "- Accuracy of the final model after 10 epochs\n",
    "- Response time of the model after 10 epochs\n",
    "\n",
    "(Possible other result to see)\n",
    "- Throughput of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For the ANOVA test\n",
    "import researchpy as rp\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# For the regression model and evaluation\n",
    "# Import the 3 regressors\n",
    "from sklearn.tree import  DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, LeaveOneOut, cross_validate, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from courier import Job, Courier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract from the log files the Accuracy and the Response Time\n",
    "\n",
    "The files are in the folder from the master server, and we get a final line in the files with\n",
    "the total wall clock time and the final accuracy\n",
    "\n",
    "The folder structure taken into consideration is the following, from the root folder for the set\n",
    "of experiments in question (2k or full) we get 4 folders, 1 for the master and 1 for each slave,\n",
    "and inside these folders we get 1 folder for each of the replications. Inside we should find the\n",
    "pickle files and the log files in case of the master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_name(exp_name: str) -> (int, int, int):\n",
    "    \"\"\"Given the name of the file give back the settings of the experiment\"\"\"\n",
    "    parts = exp_name.split('-')\n",
    "    cpu = int(parts[0].replace('cpu', '').strip())\n",
    "    batch = int(parts[1].replace('batch', '').strip())\n",
    "    njobs = int(parts[2].replace('njobs', '').strip())\n",
    "    network = str(parts[3])[3:] if len(parts) > 3 else 'lenet'\n",
    "\n",
    "    return cpu, batch, njobs, network\n",
    "\n",
    "# Read the folder os the master and search for the accuracy to build the dictionary\n",
    "# Right now for testing it just has one replication\n",
    "\n",
    "\n",
    "\n",
    "# Get a dict ready to transform into a dataframe for training of the\n",
    "# model and representation\n",
    "results = {\n",
    "    'exp_name': [],\n",
    "    'replication': [],\n",
    "    'cpu': [],\n",
    "    'batch': [],\n",
    "    'njobs': [],\n",
    "    'network': [],\n",
    "    'time': [],\n",
    "    'accuracies':[],\n",
    "    'final_accuracy': []\n",
    "}\n",
    "# Just take the final valuie for the ANOVA test\n",
    "# res_final = defaultdict(dict)\n",
    "\n",
    "\n",
    "replications = 3\n",
    "\n",
    "for r in range(1, 1+replications):\n",
    "    # This is the path we should change to either analyze the 2k or the full factorial\n",
    "    path_root_master = f'./experiments/2k/{r}/master/'\n",
    "    \n",
    "    # get the log files\n",
    "    logs = glob.glob(os.path.join(path_root_master, '*.log'))\n",
    "    \n",
    "    \n",
    "    print(f'Extracting the data from {len(logs)} files')\n",
    "    for log in logs:\n",
    "        acc = []\n",
    "        with open(log, 'r') as f:\n",
    "            for line in f:\n",
    "                if 'Top1Accuracy is Accuracy' in line:\n",
    "                    try:\n",
    "                        _accuracy = line.strip().split(']')[-1].split(',')[-1].split(':')[-1].replace(')', '')\n",
    "                        acc.append(float(_accuracy.strip()))\n",
    "                    except Exception as e:\n",
    "                        print('Error while reading accuracy',e)\n",
    "                    if len(acc) == 10:\n",
    "                        # Then save the time here\n",
    "                        time = float(line.strip().split(']')[-2].split('[')[-1].split(' ')[-1].replace('s', ''))\n",
    "                        break\n",
    "        # Add the results to the dictionary\n",
    "        variables = log.split('-')[1:]\n",
    "        if len(variables) < 4:\n",
    "            variables[-1] = variables[-1].split('.')[0]\n",
    "            variables.append('netlenet5')\n",
    "        name = '-'.join(variables).replace('.log', '')\n",
    "        cpu, batch, njobs , network= split_name(name)\n",
    "        # set all the columns\n",
    "        results['cpu'].append(cpu)\n",
    "        results['batch'].append(batch)\n",
    "        results['njobs'].append(njobs)\n",
    "        results['network'].append(network)\n",
    "        results['accuracies'].append(acc)\n",
    "        results['time'].append(time)\n",
    "        results['final_accuracy'].append(acc[-1])\n",
    "        results['exp_name'].append(name)\n",
    "        results['replication'].append(r)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df = results_df.set_index('exp_name')\n",
    "\n",
    "# We should get the last accuracy to run ANOVA but to plot this is better\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract the CPU and IO wait from the pickle files\n",
    "\n",
    "In this case we need the slave folders (we will check that the folder contains the word slave to analyze the files)\n",
    "and we will extract the pickle files from where we will get the CPU load and the IO wait percentage\n",
    "\n",
    "There is a thing to take in mind. The experiment runs for a default of 5 minutes, so if the experiment lasted\n",
    "less than 300 seconds, which we can get from the results dictionary from before, we should trim the vectors taking\n",
    "into account that each measurement is taken every 2 seconds so it just comprises that experiment and doesn't\n",
    "introduce noise from the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def coalesce_metrics(loads, replication, threshold = 1):\n",
    "    \"\"\"We have three slaves, and sometimes not all of them are working\n",
    "    if the numnber of cpus and jobs is just one\n",
    "\n",
    "    In these cases,  discard the lists with an average too close to zero,\n",
    "    if not, add the slaves together and calculate the mean at each point to\n",
    "    get the final vector.\"\n",
    "\n",
    "    :param: threshold: the threshold of utilization under which we conside\n",
    "                        that the server was not used in the experiment\"\"\"\n",
    "    # Dict to build the dataframe\n",
    "    avg_loads = {\n",
    "        'exp_name': [],\n",
    "        'replication': [],\n",
    "        'cpu_load': [],\n",
    "        'iowait': [],\n",
    "        'cpu_mean': [],\n",
    "        'iowait_mean':[]\n",
    "    }\n",
    "\n",
    "    for exp, slaves in loads.items():\n",
    "        # print(f'\\n{exp}')\n",
    "        count = 0\n",
    "        list_length = len(list(slaves['slave1'].values())[0])\n",
    "        c = np.zeros(list_length)\n",
    "        io = np.zeros(list_length)\n",
    "        for slave, metrics in slaves.items():\n",
    "\n",
    "            # if the mean is too slow discard\n",
    "            cpu_mean = np.mean(metrics['cpu'])\n",
    "            if  cpu_mean <threshold:\n",
    "                # print(f'Discarding {slave} with cpu mean =',cpu_mean)\n",
    "                pass\n",
    "            else:\n",
    "                c += metrics['cpu']\n",
    "                io += metrics['iowait']\n",
    "                count += 1\n",
    "                # print(slave, np.mean(metrics['cpu']), np.mean(metrics['iowait']))\n",
    "                # print(metrics['cpu'])\n",
    "\n",
    "        # print('dividing by ',count)\n",
    "        avg_loads['exp_name'].append(exp)\n",
    "        avg_loads['replication'].append(replication)\n",
    "        avg_loads['cpu_load'].append(c / count)\n",
    "        avg_loads['iowait'].append(io/count)\n",
    "        avg_loads['cpu_mean'].append(np.mean(c/count))\n",
    "        avg_loads['iowait_mean'].append(np.mean(io/count))\n",
    "\n",
    "    return avg_loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for replication in [1, 2, 3]:\n",
    "    print('Starting with replication ',replication)\n",
    "    root_folder_slaves = f'./experiments/2k/{replication}'\n",
    "\n",
    "    dirs = os.listdir(root_folder_slaves)\n",
    "    slave_dirs = [f for f in dirs if 'slave' in f]\n",
    "    print(slave_dirs)\n",
    "\n",
    "\n",
    "    loads = defaultdict(dict)\n",
    "    # The shape of the results is\n",
    "    # name of the file:\n",
    "    #     - slave 1:\n",
    "    #         - cpu: list\n",
    "    #         - iowait: list\n",
    "    #     - slave 2:\n",
    "    #         ...\n",
    "\n",
    "    # Do the same in all the folders\n",
    "    for folder in slave_dirs:\n",
    "        print('Processing files for the', folder)\n",
    "        # get just the pickle files\n",
    "        files = glob.glob(os.path.join(root_folder_slaves, folder, '*.pkl'))\n",
    "\n",
    "        # Extract the dictionary from each file\n",
    "        for f in files:\n",
    "\n",
    "            # Get the name of the experiment to index the other dict\n",
    "            variables = f.split('-')[1:]\n",
    "            if len(variables) < 4:\n",
    "                variables.append('netlenet5')\n",
    "            exp_name = '-'.join(variables).replace('.pkl', '')\n",
    "#             print(exp_name)\n",
    "#             print(f)\n",
    "\n",
    "            with open(f, 'rb') as pickle_file:\n",
    "                metrics = pickle.load(pickle_file)\n",
    "\n",
    "            # Now we need to check to just get the metrics concerning the experiment\n",
    "            # And not the following. We leave a margin of 10 seconds or 5 list positions just in case\n",
    "            # to not cut too tightly\n",
    "            exp_length = results_df.loc[(results_df.exp_name == exp_name) & (results_df.replication == replication), 'time'].array[0]\n",
    "            if exp_length > 300:\n",
    "                loads[exp_name][folder] = metrics\n",
    "\n",
    "            else:\n",
    "                max_list_length = math.ceil(exp_length/2) + 5\n",
    "                # print(f'Exp length was {exp_length}, so max list positions are {max_list_length}')\n",
    "                if max_list_length < len(metrics['cpu']):\n",
    "                    loads[exp_name][folder] = dict()\n",
    "                    loads[exp_name][folder]['cpu'] = metrics['cpu'][:max_list_length]\n",
    "                    loads[exp_name][folder]['iowait'] = metrics['iowait'][:max_list_length]\n",
    "                else:\n",
    "                    # Add the whole of it\n",
    "                    loads[exp_name][folder] = metrics\n",
    "\n",
    "    # Get the average usage of the experiments\n",
    "    # The resulting dict will just map each\n",
    "    # pf the experiments to its avg cpu load and\n",
    "    # iowait across all slaves\n",
    "\n",
    "    avg_loads = coalesce_metrics(loads, replication)\n",
    "    loads_df = pd.DataFrame(avg_loads)\n",
    "    print('build dataframe')\n",
    "    dfs.append(loads_df)\n",
    "\n",
    "# Concat all of the dataframes into one\n",
    "load_df = None\n",
    "\n",
    "for df in dfs:\n",
    "    load_df = df if results_df is None else pd.concat([load_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Merge the dataframes together so we get a common dataframe for all results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merge the two dataframes so we get all the data grouped by experiment number\n",
    "final_df = results_df.merge(load_df, on=['exp_name', 'replication'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If everything is already done load the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_pickle('./experiments/fullfact/complete_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plot the loads for each of the slaves for each of the experiments\n",
    "\n",
    "In some experiments some of the slaves have zero load cause they are not used, in those cases\n",
    "we should filter out the lists whose average is too close to zero. In other cases, we can just average the\n",
    "load among all of the slaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the relationship with time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(15,15))\n",
    "\n",
    "sns.barplot(x='njobs', y='time', hue='batch', data=final_df, ax=ax1)\n",
    "sns.barplot(x='cpu', y='time', hue='batch', data=final_df, ax=ax2)\n",
    "\n",
    "final_df['processes'] = final_df['cpu'] * final_df['njobs']\n",
    "sns.barplot(x='processes', y='time', hue='network',data=final_df, ax=ax3)\n",
    "sns.barplot(x='batch', y='time', hue='network', data=final_df, ax=ax4);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### See the relationships with the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(15,15))\n",
    "\n",
    "sns.violinplot(x='njobs', y='final_accuracy', data=final_df, ax=ax1)\n",
    "sns.violinplot(x='cpu', y='final_accuracy', data=final_df, ax=ax2)\n",
    "\n",
    "final_df['processes'] = final_df['cpu'] * final_df['njobs']\n",
    "sns.violinplot(x='processes', y='final_accuracy', data=final_df, ax=ax3)\n",
    "sns.barplot(x='batch', y='final_accuracy', hue='network', data=final_df, ax=ax4);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### See the relationship with the cpu load and iowait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(15,15))\n",
    "\n",
    "sns.violinplot(x='njobs', y='cpu_mean', data=final_df, ax=ax1)\n",
    "sns.violinplot(x='cpu', y='cpu_mean', data=final_df, ax=ax2)\n",
    "\n",
    "final_df['processes'] = final_df['cpu'] * final_df['njobs']\n",
    "sns.barplot(x='processes', y='cpu_mean', hue='network', data=final_df, ax=ax3)\n",
    "sns.barplot(x='batch', y='cpu_mean', hue = 'cpu', data=final_df, ax=ax4);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(15,15))\n",
    "\n",
    "sns.barplot(x='njobs', y='iowait_mean', data=final_df, ax=ax1)\n",
    "sns.barplot(x='cpu', y='iowait_mean', data=final_df, ax=ax2)\n",
    "\n",
    "final_df['processes'] = final_df['cpu'] * final_df['njobs']\n",
    "sns.lineplot(x='processes', y='iowait_mean', data=final_df, ax=ax3)\n",
    "sns.barplot(x='batch', y='iowait_mean', data=final_df, ax=ax4);\n",
    "\n",
    "\n",
    "final_df.groupby('njobs').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run some statistical tests to see if the batch influences any of the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run for batch with the IO, CPU...\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "import itertools\n",
    "\n",
    "def njobs_significance_test(column: str):\n",
    "\n",
    "    print('Starting jobs significance test for column', column)\n",
    "\n",
    "    # Divide in batches\n",
    "    n0 = final_df[final_df.njobs == 1][column]\n",
    "    n1 = final_df[final_df.njobs == 3][column]\n",
    "    n2 = final_df[final_df.njobs == 5][column]\n",
    "\n",
    "    jobs = {1: n0,\n",
    "               3:n1,\n",
    "               5:n2\n",
    "    }\n",
    "\n",
    "    # Calculate p-values\n",
    "    combinations = itertools.combinations(list(jobs.keys()), 2)\n",
    "    for i, j in combinations:\n",
    "        # Calculate the test for those two values\n",
    "        _, p_val = mannwhitneyu(jobs[i], jobs[j])\n",
    "        print(f'P-value of {i} and {j} is {p_val} --> {\"RELEVANT\" if p_val < 0.05 else \"NON_RELEVANT\"}')\n",
    "    print()\n",
    "\n",
    "def cpu_significance_test(column: str):\n",
    "\n",
    "    print('Starting cpu significance test for column', column)\n",
    "\n",
    "    # Divide in batches\n",
    "    c0 = final_df[final_df.cpu == 1][column]\n",
    "    c1 = final_df[final_df.cpu == 2][column]\n",
    "    c2 = final_df[final_df.cpu == 4][column]\n",
    "    c3 = final_df[final_df.cpu == 8][column]\n",
    "\n",
    "    cpus = {1: c0,\n",
    "               2:c1,\n",
    "               4:c2,\n",
    "               8:c3\n",
    "    }\n",
    "\n",
    "    # Calculate p-values\n",
    "    combinations = itertools.combinations(list(cpus.keys()), 2)\n",
    "    for i, j in combinations:\n",
    "        # Calculate the test for those two values\n",
    "        _, p_val = mannwhitneyu(cpus[i], cpus[j])\n",
    "        print(f'P-value of {i} and {j} is {p_val} --> {\"RELEVANT\" if p_val < 0.05 else \"NON_RELEVANT\"}')\n",
    "    print()\n",
    "\n",
    "\n",
    "def batch_significance_test(column: str):\n",
    "\n",
    "    print('Starting batch significance test for column', column)\n",
    "\n",
    "    # Divide in batches\n",
    "    b0 = final_df[final_df.batch == 64][column]\n",
    "    b1 = final_df[final_df.batch == 128][column]\n",
    "    b2 = final_df[final_df.batch == 256][column]\n",
    "    b3 = final_df[final_df.batch == 512][column]\n",
    "\n",
    "    batches = {64: b0,\n",
    "               128:b1,\n",
    "               256:b2,\n",
    "               512:b3\n",
    "    }\n",
    "\n",
    "    # Calculate p-values\n",
    "    combinations = itertools.combinations([64,128,256,512], 2)\n",
    "    for i, j in combinations:\n",
    "        # Calculate the test for those two values\n",
    "        _, p_val = mannwhitneyu(batches[i], batches[j])\n",
    "        print(f'P-value of {i} and {j} is {p_val} --> {\"RELEVANT\" if p_val < 0.05 else \"NON_RELEVANT\"}')\n",
    "    print()\n",
    "\n",
    "for col in ['time', 'final_accuracy', 'cpu_mean', 'iowait_mean']:\n",
    "    print(f'--------- {col.upper()} ------------')\n",
    "    batch_significance_test(col)\n",
    "    njobs_significance_test(col)\n",
    "    cpu_significance_test(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Try to calculate the service rate of the disk based on the number of jobs and the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Tcpu = 0.045\n",
    "mu_cpu = 1/Tcpu\n",
    "num_examples = 60000\n",
    "num_epochs = 10\n",
    "\n",
    "mus = {\n",
    "    'batch':[],\n",
    "    'cpu':[],\n",
    "    'processes':[],\n",
    "    'njobs':[],\n",
    "    'mu':[],\n",
    "    'time':[]\n",
    "}\n",
    "\n",
    "for i, row in final_df.iterrows():\n",
    "    # Calculate the new numbers\n",
    "    l_disk = max(1, (row.njobs * row.cpu)/3) * mu_cpu\n",
    "    T = row.time\n",
    "    Nt = (num_examples/row.batch) * num_epochs\n",
    "    mu_disk = (1/((T/Nt) - Tcpu)) + l_disk\n",
    "    \n",
    "    mus['batch'].append(row.batch)\n",
    "    mus['cpu'].append(row.cpu)\n",
    "    mus['processes'].append(row.njobs * row.cpu)\n",
    "    mus['njobs'].append(row.njobs)\n",
    "    mus['mu'].append(mu_disk)\n",
    "    mus['time'].append(row.time)\n",
    "\n",
    "    \n",
    "df_mu = pd.DataFrame(mus)\n",
    "\n",
    "df_mu.groupby(['batch', 'njobs']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(3, figsize=(15, 10))\n",
    "\n",
    "sns.lineplot(x='batch', y='mu', data=df_mu, ax=ax1)\n",
    "sns.lineplot(x='processes', y='mu', data=df_mu, ax=ax2)\n",
    "sns.lineplot(x='cpu', y='mu', data=df_mu, ax=ax3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tcpu = 0.045\n",
    "mu_cpu = 1/Tcpu\n",
    "num_examples = 60000\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# See how well it does just on the training dataframe\n",
    "labels_time = final_df['time']\n",
    "\n",
    "# Divide into train and test set\n",
    "train_df = final_df\n",
    "x_train, x_test, y_train_mu, y_test_mu = train_test_split(df_mu, df_mu['mu'], test_size=0.2, random_state = 42)\n",
    "\n",
    "res = {\n",
    "    'expected':[],\n",
    "    'actual':[]\n",
    "}\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train[['batch', 'processes']], y_train_mu)\n",
    "\n",
    "\n",
    "# Time for all the rows in the test set \n",
    "for _, row in x_test.iterrows():\n",
    "    # Measured time\n",
    "    T_true = row.time\n",
    "    \n",
    "    # Apply the formula\n",
    "    l_disk = max(1, row.processes/3) * mu_cpu\n",
    "    mu_disk = lr.predict([[row.batch,row.processes]])[0]\n",
    "    Nt = (num_examples/row.batch) * num_epochs\n",
    "    Tdisk = 1/(mu_disk - l_disk)\n",
    "    \n",
    "    T = Nt * (Tcpu + Tdisk)\n",
    "    res['expected'].append(T)\n",
    "    res['actual'].append(T_true)\n",
    "    \n",
    "res_df = pd.DataFrame(res)\n",
    "\n",
    "# display(res_df)\n",
    "    \n",
    "# Get the MSE and the R2 score of the model\n",
    "# 2) Get the predictions for the accuracy\n",
    "mse_t = mean_squared_error(res_df['actual'], res_df['expected'])\n",
    "r2_t = r2_score(res_df['actual'], res_df['expected'])\n",
    "\n",
    "print(f'TIME: MSE = {mse_t}, R2: {r2_t}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.scatter(x_test['processes'], res_df['actual'], label=\"True values\")\n",
    "plt.scatter(x_test['processes'], res_df['expected'], label=\"Predicted values\")\n",
    "\n",
    "plt.xlabel('Processes')\n",
    "plt.ylabel('Time(s)')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the ANOVA tests on this data \n",
    "\n",
    "We want to see how factors like number of CPUs, Jobs, and Batch influence all the others\n",
    "- Accuracy\n",
    "- E(T)\n",
    "- CPU Usage\n",
    "- IO Wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ANOVA(df: pd.DataFrame, y: str, use_all = False,verbose=False):\n",
    "    \"\"\"Run the ANOVA analysis with the cpu, batch and njobs columns for the \n",
    "    given output variable\"\"\"\n",
    "    \n",
    "    # If use all is true we use all the variables to check either accuracy and time\n",
    "    # including also the iowait and the cpu to see what fully influences the stuff\n",
    "    \n",
    "    \n",
    "    if not use_all:\n",
    "        # Plot the summary dataframe\n",
    "        if verbose:\n",
    "            display(rp.summary_cont(df.groupby(['cpu', 'batch', 'njobs', 'network']))[y])\n",
    "\n",
    "        model = ols(f'{y} ~ cpu*batch*njobs*C(network)', df).fit()\n",
    "        \n",
    "    else:\n",
    "        if y not in ['final_accuracy', 'time']:\n",
    "            raise ValueError('When use_all = True we predict either final_accuracy or time, not', y)\n",
    "        if verbose:\n",
    "            display(rp.summary_cont(df.groupby(['cpu', 'batch', 'njobs', 'cpu_mean', 'iowait_mean']))[y])\n",
    "\n",
    "        model = ols(f'{y} ~ cpu*batch*njobs*cpu_mean*iowait_mean', df).fit()\n",
    "        \n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Overall model F({model.df_model: .0f},{model.df_resid: .0f}) = {model.fvalue: .3f}, p = {model.f_pvalue: .4f}\")\n",
    "        display(model.summary())\n",
    "    \n",
    "    res = sm.stats.anova_lm(model, typ=2)\n",
    "    \n",
    "    return res, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Simply change the output value for the y to the variable that you want to study\n",
    "# - iowait_mean\n",
    "# - final_accuracy\n",
    "# - time\n",
    "# - cpu_mean\n",
    "res, model = ANOVA(final_df, y='time', verbose=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how well the model can fit the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we get the model we can try to fit in on the x_train\n",
    "labels_time = final_df['time']\n",
    "labels_acc = final_df['final_accuracy']\n",
    "\n",
    "train_df = final_df\n",
    "train_df['network'] = train_df['network'].apply(lambda net: 1 if net == 'lenet5' else -1)\n",
    "x_train, x_test, y_train_time, y_test_time = train_test_split(train_df, labels_time, test_size=0.2, random_state = 42)\n",
    "x_train, x_test, y_train_acc, y_test_acc = train_test_split(train_df, labels_acc, test_size=0.2, random_state = 42)\n",
    "\n",
    "_, model_time = ANOVA(x_train, y='time')\n",
    "_, model_acc = ANOVA(x_train, y='final_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MSE and the R2 score\n",
    "y_pred_time_anova = model_time.predict(x_test)\n",
    "y_pred_acc_anova = model_acc.predict(x_test)\n",
    "\n",
    "# 1) Get the predictions for the time\n",
    "mse_t = mean_squared_error(y_pred_time_anova, y_test_time)\n",
    "r2_t = r2_score(y_pred_time_anova, y_test_time)\n",
    "\n",
    "print(f'TIME: MSE = {mse_t}, R2: {r2_t}')\n",
    "\n",
    "\n",
    "# 2) Get the predictions for the accuracy\n",
    "mse_acc = mean_squared_error(y_pred_acc_anova, y_test_acc)\n",
    "r2_acc = r2_score(y_pred_acc_anova, y_test_acc)\n",
    "\n",
    "print(f'ACC: MSE = {mse_acc}, R2: {r2_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pritn the predictions and the true values\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, figsize=(15, 10))\n",
    "\n",
    "ax1.scatter(x_test.processes, y_test_time, 'ro', label='True labels')\n",
    "ax1.scatter(x_test.processes, y_pred_time_anova, label='Predicted values')\n",
    "\n",
    "ax1.set_xlabel('Number of processes')\n",
    "ax1.set_ylabel('Time (s)')\n",
    "\n",
    "ax2.scatter(x_test.processes, y_test_acc, label='True labels')\n",
    "ax2.scatter(x_test.processes, y_pred_acc_anova, label='Predicted values')\n",
    "\n",
    "ax2.set_xlabel('Number of processes')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fit the Data with the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Create the interaction column\n",
    "final_df['processes'] = final_df['cpu'] * final_df['njobs']\n",
    "\n",
    "# Get the data split into train and test\n",
    "train_df = final_df[['processes', 'cpu', 'njobs', 'batch', 'network']]\n",
    "train_df['network'] = train_df['network'].apply(lambda net: 1 if net == 'lenet5' else -1)\n",
    "\n",
    "labels_time = final_df['time']\n",
    "labels_acc = final_df['final_accuracy']\n",
    "\n",
    "train_df = scaler.fit_transform(train_df)\n",
    "train_df\n",
    "\n",
    "# Fit just the Random Forest regressor with a GridSearch crossval to find the best hyperparams\n",
    "x_train, x_test, y_train_acc, y_test_acc = train_test_split(train_df, labels_acc, test_size=0.2, random_state = 42)\n",
    "x_train, x_test, y_train_time, y_test_time = train_test_split(train_df, labels_time, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reg_acc = RandomForestRegressor(random_state=42)\n",
    "reg_time = RandomForestRegressor(random_state=42)\n",
    "\n",
    "params_rf = {\n",
    "    'n_estimators': [50, 100, 150, 200, 500, 1000, 2000],\n",
    "    'max_features': ['auto', 'log2', 'sqrt'],\n",
    "    'max_depth': range(4,10),\n",
    "    'criterion': ['mse', 'mae']\n",
    "    \n",
    "}\n",
    "\n",
    "# First cross validation\n",
    "print('Fitting the time...')\n",
    "cv_time = GridSearchCV(estimator=reg_time, param_grid=params_rf, n_jobs=8, cv=5, verbose=3)\n",
    "cv_time.fit(x_train, y_train_time)\n",
    "\n",
    "\n",
    "# Second cross val\n",
    "print('Fitting the accuracy')\n",
    "cv_acc = GridSearchCV(estimator=reg_acc, param_grid=params_rf, n_jobs=8, cv=5, verbose=3)\n",
    "cv_acc.fit(x_train, y_train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fit the regressors on the accuracy and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_time.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1) Fit on the time\n",
    "r_t = cv_time.best_estimator_.fit(x_train, y_train_time)\n",
    "\n",
    "# test the performance on the train and the test set\n",
    "y_pred = r_t.predict(x_test)\n",
    "\n",
    "err = mean_squared_error(y_pred, y_test_time)\n",
    "\n",
    "print('Mean Squared Error with the test set is for TIME', err)\n",
    "\n",
    "\n",
    "# 2) Fit on the acc\n",
    "r_acc = cv_acc.best_estimator_.fit(x_train, y_train_acc)\n",
    "\n",
    "# test the performance on the train and the test set\n",
    "y_pred = r_acc.predict(x_test)\n",
    "err = mean_squared_error(y_pred, y_test_acc)\n",
    "\n",
    "print('Mean Squared Error with the test set is for ACCURACY', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the resulting best hyperparams from the GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the regressor for time with the best params\n",
    "r_t = RandomForestRegressor(n_estimators=2000, \n",
    "                            criterion='mae', \n",
    "                            max_features='auto', \n",
    "                            max_depth=6, \n",
    "                            random_state=42, n_jobs=6).fit(x_train, y_train_time)\n",
    "\n",
    "# Fit the regressor for Accuracy with the best params\n",
    "r_acc= RandomForestRegressor(n_estimators=150, \n",
    "                             criterion='mae', \n",
    "                             max_features='auto', \n",
    "                             max_depth=9, \n",
    "                             random_state=42, n_jobs=6).fit(x_train, y_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions from the models\n",
    "\n",
    "# 1) Get the performance from time\n",
    "y_pred_time = r_t.predict(x_test)\n",
    "\n",
    "mse_t = mean_squared_error(y_test_time, y_pred_time)\n",
    "r2_t = r2_score(y_test_time, y_pred_time)\n",
    "print(f'TIME: MSE = {mse_t}, R2: {r2_t}')\n",
    "\n",
    "# 2) Get the performance for the accuracy\n",
    "y_pred_acc = r_acc.predict(x_test)\n",
    "\n",
    "mse_acc = mean_squared_error(y_test_acc, y_pred_acc)\n",
    "r2_acc = r2_score(y_test_acc, y_pred_acc)\n",
    "print(f'ACC: MSE = {mse_acc}, R2: {r2_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pritn the predictions and the true values\n",
    "f, (ax1, ax2) = plt.subplots(2, figsize=(10, 10))\n",
    "\n",
    "ax1.scatter(x_test[:,0], y_test_time, label='True labels')\n",
    "ax1.errorbar(x_test[:,0], y_pred_time, yerr=y_test_time-y_pred_time, fmt='o', marker='^', c='r', label='Predicted values')\n",
    "\n",
    "ax1.set_xlabel('Normalized number of processes', fontsize=17)\n",
    "ax1.set_ylabel('Time (s)', fontsize=17)\n",
    "\n",
    "ax2.scatter(x_test[:,0], y_test_acc,label='True labels')\n",
    "ax2.errorbar(x_test[:,0], y_pred_acc, yerr=y_test_acc-y_pred_acc, fmt='o', marker='^', c='r', label='Predicted values')\n",
    "\n",
    "ax2.set_xlabel('Normalized number of processes', fontsize=15)\n",
    "ax2.set_ylabel('Accuracy', fontsize=17)\n",
    "\n",
    "ax1.legend(fontsize=17)\n",
    "ax2.legend(fontsize=17)\n",
    "\n",
    "plt.savefig('./figures/random_forest_plot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, imp in zip(['processes', 'cpu', 'njobs', 'batch', 'network'],r_t.feature_importances_ ):\n",
    "    print(f, imp)\n",
    "    \n",
    "f, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(10,7))\n",
    "ax1.bar(['processes', 'cpu', 'njobs', 'batch', 'network'], r_t.feature_importances_)\n",
    "ax2.bar(['processes', 'cpu', 'njobs', 'batch', 'network'], r_acc.feature_importances_)\n",
    "ax1.tick_params(axis='x',rotation=40)\n",
    "ax1.tick_params(axis='both',labelsize=13)\n",
    "\n",
    "ax2.tick_params(axis='x',rotation=40)\n",
    "ax2.tick_params(axis='both',labelsize=13)\n",
    "\n",
    "\n",
    "ax1.set_ylabel('Importance', fontsize=17)\n",
    "ax1.set_xlabel('Features Resp. Time', fontsize=17)\n",
    "ax2.set_xlabel('Features Accuracy', fontsize=17)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./figures/importance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Courier experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Courier\n",
    "\n",
    "with open('./courier.pkl', 'rb') as f:\n",
    "    courier = pickle.load(f)\n",
    "    \n",
    "courier   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_root_path = './experiments/courier'\n",
    "exp_path = './experiment_designs/courier_exp.csv'\n",
    "\n",
    "# Load the experiments\n",
    "exps = pd.read_csv(exp_path)\n",
    "\n",
    "def split_name(exp_name: str) -> (int, int, int):\n",
    "    \"\"\"Given the name of the file give back the settings of the experiment\"\"\"\n",
    "    parts = exp_name.split('-')\n",
    "    cpu = int(parts[0].replace('cpu', '').strip())\n",
    "    batch = int(parts[1].replace('batch', '').strip())\n",
    "    njobs = int(parts[2].replace('njobs', '').strip())\n",
    "    network = str(parts[3])[3:] if len(parts) > 3 else 'lenet'\n",
    "    \n",
    "    return cpu, batch, njobs, network\n",
    "    \n",
    "\n",
    "\n",
    "def read_logs(folder:str):\n",
    "    results = {\n",
    "        'exp_name': [],\n",
    "        'replication': [],\n",
    "        'cpu': [],\n",
    "        'batch': [],\n",
    "        'njobs': [],\n",
    "        'network': [],\n",
    "        'time': [],\n",
    "        'accuracies':[],\n",
    "        'final_accuracy': []\n",
    "    }\n",
    "\n",
    "\n",
    "    # 1) Load the results from the courier part of the experiments\n",
    "    # We need to do as with the normal experiments for the ANOVA and extract the accuracies and time\n",
    "    for replication in [1,2,3]:\n",
    "        logs = glob.glob(os.path.join(results_root_path, folder, str(replication), '*.log'))\n",
    "        \n",
    "\n",
    "        print(f'Extracting the data from {len(logs)} files')\n",
    "        for log in logs:\n",
    "            acc = []\n",
    "            with open(log, 'r') as f:\n",
    "                for line in f:\n",
    "                    if 'Top1Accuracy is Accuracy' in line:\n",
    "                        try:\n",
    "                            _accuracy = line.strip().split(']')[-1].split(',')[-1].split(':')[-1].replace(')', '')\n",
    "                            acc.append(float(_accuracy.strip()))\n",
    "                        except Exception as e:\n",
    "                            print('Error while reading accuracy',e)\n",
    "                        if len(acc) == 10:\n",
    "                            # Then save the time here\n",
    "                            time = float(line.strip().split(']')[-2].split('[')[-1].split(' ')[-1].replace('s', ''))\n",
    "                            break\n",
    "            # Add the results to the dictionary\n",
    "            variables = log.split('-')[1:-1]\n",
    "            if len(variables) < 4:\n",
    "                variables[-1] = variables[-1].split('.')[0]\n",
    "                variables.append('netlenet5')\n",
    "            name = '-'.join(variables).replace('.log', '')\n",
    "            cpu, batch, njobs , network= split_name(name)\n",
    "            # set all the columns\n",
    "            results['cpu'].append(cpu)\n",
    "            results['batch'].append(batch)\n",
    "            results['njobs'].append(njobs)\n",
    "            results['network'].append(network)\n",
    "            results['accuracies'].append(acc)\n",
    "            results['time'].append(time)\n",
    "            results['final_accuracy'].append(acc[-1])\n",
    "            results['exp_name'].append(name)\n",
    "            results['replication'].append(replication)\n",
    "            \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_courier_predictions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Gets the predictions for what courier predicts based on the jobs, cpu for a dataframe\"\"\"\n",
    "    preds = {\n",
    "        'exp_name':[],\n",
    "        'best_batch':[],\n",
    "        'predicted_time':[],\n",
    "        'predicted_accuracy':[],\n",
    "        'cpu':[],\n",
    "        'njobs':[],\n",
    "        'latency':[]\n",
    "    }\n",
    "    \n",
    "    df['network'] = df['network'].apply(lambda n: 1 if n == 'lenet5' else -1)\n",
    "    \n",
    "    for i, row in df[df.index<5].iterrows():\n",
    "        # Get the prediction from courier\n",
    "        latency = exps.iloc[i].latency\n",
    "        j = Job(cpu=row.cpu, njobs=row.njobs, network=row.network)\n",
    "        print(f'Row {i}, job = {j}')\n",
    "        best_batch, (acc, t) = courier.optimize(j, latency)\n",
    "        preds['exp_name'].append(row.exp_name)\n",
    "        preds['best_batch'].append(best_batch)\n",
    "        preds['predicted_time'].append(t)\n",
    "        preds['predicted_accuracy'].append(acc)\n",
    "        preds['cpu'].append(row.cpu)\n",
    "        preds['njobs'].append(row.njobs)\n",
    "        preds['latency'].append(latency)\n",
    "    \n",
    "    return pd.DataFrame(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the results from the courier model and the random model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_courier = read_logs('courier')\n",
    "results_random = read_logs('random')\n",
    "preds = get_courier_predictions(results_courier)\n",
    "\n",
    "# fix error in reading experiment 4 in replication 3\n",
    "results_courier.at[13, 'time'] =  126.568445561\n",
    "results_courier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes by name\n",
    "df_c = results_courier.merge(preds, on='exp_name')\n",
    "df_r=results_random.merge(preds, on=['cpu', 'njobs']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create extra columns for the MSE and the SLO difference\n",
    "\n",
    "The SLO difference is given by Response Time - Latency, it should be negative to indicate that the\n",
    "model adhered to the SLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the error in time prediction and get the error in accuracy\n",
    "df_c['error_time'] = (df_c['time'] - df_c['predicted_time'])**2\n",
    "df_c['error_acc'] = (df_c['final_accuracy'] - df_c['predicted_accuracy'])**2\n",
    "\n",
    "df_r['error_time'] = (df_r['time'] - df_r['predicted_time'])**2\n",
    "df_r['error_acc'] = (df_r['final_accuracy'] - df_r['predicted_accuracy'])**2\n",
    "\n",
    "\n",
    "df_c['slo'] = df_c['time'] - df_c['latency']\n",
    "df_r['slo'] = df_r['time'] - df_r['latency']\n",
    "\n",
    "\n",
    "df_c_means = df_c.groupby(['exp_name']).mean().reset_index()\n",
    "df_r_means = df_r.groupby(['cpu', 'njobs', 'network']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent the plots\n",
    "\n",
    "This plots are the ones used for the final section of the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the mean squared error between time and predicted\n",
    "print('Errors in time')\n",
    "print(mean_squared_error(df_c['time'], df_c['predicted_time']))\n",
    "print(mean_squared_error(df_r['time'], df_r['predicted_time']))\n",
    "\n",
    "# Print the mean squared error in accuracy\n",
    "print('\\n\\nerrors in accuracy')\n",
    "print(mean_squared_error(df_c['final_accuracy'], df_c['predicted_accuracy']))\n",
    "print(mean_squared_error(df_r['final_accuracy'], df_r['predicted_accuracy']))\n",
    "\n",
    "plt.rc('font', size=12)\n",
    "\n",
    "sns.set_palette('muted')\n",
    "f, (ax1, ax2) = plt.subplots(2, figsize=(12,10), sharex=True)\n",
    "\n",
    "sns.barplot(x=df_c_means.index, y=np.sqrt(df_c_means.error_time), data=df_c_means, ax=ax1, palette='muted')\n",
    "sns.barplot(x=df_c_means.index, y=np.sqrt(df_c_means.error_acc), data=df_c_means, ax=ax2, palette='muted')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax2.set_xlabel('Experiment number')\n",
    "ax1.set_ylabel('AE Response Time')\n",
    "ax2.set_ylabel('AE Accuracy')\n",
    "\n",
    "\n",
    "plt.savefig('./figures/courier_performance.pdf')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get a significance test\n",
    "from scipy.stats import shapiro, mannwhitneyu\n",
    "\n",
    "_,p_t = mannwhitneyu(df_c['error_time'], df_r['error_time'])\n",
    "_,p_acc = mannwhitneyu(df_c['error_acc'], df_r['error_acc'])\n",
    "(p_t, p_acc)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6), sharey=True, sharex=True)\n",
    "sns.barplot(x=df_c_means.index, y=df_c_means.slo, data=df_c, ax=ax1)\n",
    "sns.barplot(x=df_r_means.index, y=df_r_means.slo, data=df_r, ax=ax2)\n",
    "ax1.set_xlabel('Experiment Number')\n",
    "ax2.set_xlabel('Experiment Number')\n",
    "ax1.set_ylabel('MAE Response Time')\n",
    "\n",
    "plt.savefig('./figures/courier-vs-random_slo.pdf')\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6), sharey=True)\n",
    "sns.barplot(x=df_c_means.index, y=df_c_means.final_accuracy, data=df_c, ax=ax1)\n",
    "sns.barplot(x=df_r_means.index, y=df_r_means.final_accuracy, data=df_r, ax=ax2)\n",
    "ax1.set_ylim([0.88, 0.96])\n",
    "\n",
    "ax1.set_xlabel('Experiment Number')\n",
    "ax2.set_xlabel('Experiment Number')\n",
    "ax1.set_ylabel('MAE Accuracy')\n",
    "\n",
    "plt.savefig('./figures/courier-vs-random_accuracy.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
